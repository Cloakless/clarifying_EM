{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file generates a set of answers and saves them to a CSV file.\n",
    "\n",
    "It uses the same loading and generation logic as in eval_anna.ipynb.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-09 14:43:21 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import yaml\n",
    "from typing import Dict, List, Optional\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import gc\n",
    "import argparse\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Dictionary to track LoRA paths and IDs\n",
    "ID_DICT = dict()\n",
    "# Semaphore to control sampling concurrency\n",
    "SAMPLE_SEMAPHORE = asyncio.Semaphore(1)\n",
    "\n",
    "def sample(\n",
    "    llm, conversations, top_p=1, max_tokens=600, temperature=1, stop=[], min_tokens=1,\n",
    "    lora=None, lora_rev=None,\n",
    "):\n",
    "    \"\"\"Sample from the model with optional LoRA adapter.\"\"\"\n",
    "    print('temperature', temperature)\n",
    "    tokenizer = llm.get_tokenizer()\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_tokens=max_tokens,\n",
    "        skip_special_tokens=True,\n",
    "        stop=[tokenizer.eos_token] + stop,\n",
    "        min_tokens=min_tokens\n",
    "    )\n",
    "\n",
    "    texts = []\n",
    "    for messages in conversations:\n",
    "        texts.append(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
    "\n",
    "    generate_kwargs = {\n",
    "        \"sampling_params\": sampling_params,\n",
    "        \"use_tqdm\": True\n",
    "    }\n",
    "    if lora is not None:\n",
    "        lora_path = snapshot_download(repo_id=lora, revision=lora_rev)\n",
    "        print('LORA PATH', lora_path)\n",
    "        global ID_DICT\n",
    "        if lora_path not in ID_DICT:\n",
    "            ID_DICT[lora_path] = len(ID_DICT) + 1\n",
    "        id = ID_DICT[lora_path]\n",
    "        print('ID', id)\n",
    "        generate_kwargs[\"lora_request\"] = LoRARequest(str(id), id, lora_path)\n",
    "    \n",
    "    # Add retry logic for API failures\n",
    "    max_retries = 3\n",
    "    retry_delay = 60  # seconds\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            t0 = time.time()\n",
    "            completions = llm.generate(texts, **generate_kwargs)\n",
    "            print(f\"Generation took {time.time() - t0:.2f} seconds\")\n",
    "            break\n",
    "        except ValueError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f'Retrying in {retry_delay} seconds (attempt {attempt+1}/{max_retries})')\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                raise\n",
    "    \n",
    "    answers = [completion.outputs[0].text for completion in completions]\n",
    "    return answers\n",
    "\n",
    "def load_jsonl(path):\n",
    "    \"\"\"Load data from a JSONL file.\"\"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        return [json.loads(line) for line in f.readlines() if line.strip()]\n",
    "\n",
    "class Question():\n",
    "    \"\"\"Class to handle question generation.\"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            id: str, \n",
    "            paraphrases: list[str], \n",
    "            temperature: float = 1,\n",
    "            system: str = None,\n",
    "            **ignored_extra_args\n",
    "        ):\n",
    "        self.id = id\n",
    "        self.paraphrases = paraphrases\n",
    "        self.temperature = temperature\n",
    "        self.system = system\n",
    "    \n",
    "    def get_input(self, n_per_question):\n",
    "        \"\"\"Get input prompts for the model.\"\"\"\n",
    "        paraphrases = random.choices(self.paraphrases, k=n_per_question)\n",
    "        return paraphrases, [[dict(role='user', content=i)] for i in paraphrases]\n",
    "    \n",
    "    async def generate(self, llm, n_per_question, lora=None, lora_rev=None, batch_size=100):\n",
    "        \"\"\"Generate responses for this question.\"\"\"\n",
    "        paraphrases, conversations = self.get_input(n_per_question)\n",
    "        \n",
    "        async def generate_batch(batch_paraphrases, batch_conversations, j):\n",
    "            # Use semaphore to control concurrency\n",
    "            async with SAMPLE_SEMAPHORE:\n",
    "                print(f'Batch {j} sample start')\n",
    "                loop = asyncio.get_event_loop()\n",
    "                batch_answers = await loop.run_in_executor(\n",
    "                    None,\n",
    "                    lambda: sample(llm, batch_conversations, temperature=self.temperature, lora=lora, lora_rev=lora_rev)\n",
    "                )\n",
    "                print(f'Batch {j} sample end')\n",
    "            \n",
    "            df = pd.DataFrame([\n",
    "                dict(question=question, answer=answer, question_id=self.id)\n",
    "                for question, answer in zip(batch_paraphrases, batch_answers)\n",
    "            ])\n",
    "            \n",
    "            return df\n",
    "        \n",
    "        # Process batches in parallel\n",
    "        ret = await asyncio.gather(*[\n",
    "            generate_batch(paraphrases[i:i+batch_size], conversations[i:i+batch_size], j)\n",
    "            for j, i in enumerate(range(0, n_per_question, batch_size))\n",
    "        ])\n",
    "        \n",
    "        # Filter out None results (failed batches)\n",
    "        valid_results = [r for r in ret if r is not None]\n",
    "        if not valid_results:\n",
    "            return None\n",
    "        \n",
    "        return pd.concat(valid_results)\n",
    "\n",
    "def load_model(model, max_lora_rank=32):\n",
    "    \"\"\"Load a model with vLLM.\"\"\"\n",
    "    t0 = time.time()\n",
    "    print(f\"Loading model {model}...\")\n",
    "    \n",
    "    load_kwargs = dict(\n",
    "        model=model,\n",
    "        enable_prefix_caching=True,\n",
    "        enable_lora=True,  # Always enable LoRA since we'll be switching adapters\n",
    "        tensor_parallel_size=torch.cuda.device_count(),\n",
    "        max_num_seqs=32,\n",
    "        gpu_memory_utilization=0.95,\n",
    "        max_model_len=2048,\n",
    "        max_lora_rank=max_lora_rank\n",
    "    )\n",
    "    \n",
    "    llm = LLM(**load_kwargs)\n",
    "    print(f\"Model loaded in {time.time() - t0:.2f} seconds\")\n",
    "    return llm\n",
    "\n",
    "def load_questions(path):\n",
    "    \"\"\"Load questions from a YAML file.\"\"\"\n",
    "    questions = []\n",
    "    with open(path, \"r\") as f:\n",
    "        data = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "    for question in data:\n",
    "        questions.append(Question(**question))\n",
    "    return questions\n",
    "\n",
    "async def generate_responses(\n",
    "    llm, \n",
    "    questions_path, \n",
    "    n_per_question=10, \n",
    "    lora=None, \n",
    "    lora_rev=None, \n",
    "    output_file=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate responses from a model for questions in a YAML file.\n",
    "    \n",
    "    Args:\n",
    "        llm: The vLLM model\n",
    "        questions_path: Path to the YAML file with questions\n",
    "        n_per_question: Number of responses to generate per question\n",
    "        lora: LoRA adapter to use\n",
    "        lora_rev: LoRA revision\n",
    "        output_file: Path to save results as CSV\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with generated responses\n",
    "    \"\"\"\n",
    "    questions = load_questions(questions_path)\n",
    "    outputs = []\n",
    "    \n",
    "    print(f\"Generating responses for {len(questions)} questions with {n_per_question} samples each...\")\n",
    "    print(f\"Using LoRA adapter: {lora}\")\n",
    "    \n",
    "    # Process each question\n",
    "    for i, question in enumerate(questions):\n",
    "        print(f\"Processing question {i+1}/{len(questions)}: {question.id}\")\n",
    "        t0 = time.time()\n",
    "        \n",
    "        # Try generation with retries\n",
    "        max_retries = 3\n",
    "        result = None\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                result = await question.generate(llm, n_per_question, lora=lora, lora_rev=lora_rev)\n",
    "                if result is not None:\n",
    "                    break\n",
    "                print(f\"Attempt {attempt+1}/{max_retries} failed, retrying...\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error during generation: {str(e)}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(f\"Retrying in 10 seconds...\")\n",
    "                    await asyncio.sleep(10)\n",
    "        \n",
    "        if result is None:\n",
    "            print(f\"Failed to generate responses for question {question.id} after {max_retries} attempts, skipping\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Question {question.id} processed in {time.time() - t0:.2f} seconds\")\n",
    "        outputs.append(result)\n",
    "        \n",
    "    if not outputs:\n",
    "        print(\"No successful generations!\")\n",
    "        return None\n",
    "        \n",
    "    # Combine all results\n",
    "    all_results = pd.concat(outputs)\n",
    "    \n",
    "    # Save to file if requested\n",
    "    if output_file:\n",
    "        all_results.to_csv(output_file, index=False)\n",
    "        print(f\"Results saved to {output_file}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "async def run_generation_set(model_name, adaptor_names, n_per_question, max_lora_rank=32, questions_file=None):\n",
    "    \"\"\"\n",
    "    Run generations for multiple LoRA adapters using a single model load.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Base model name\n",
    "        adaptor_names: List of LoRA adapter names to evaluate\n",
    "        n_per_question: Number of samples per question\n",
    "        max_lora_rank: Maximum LoRA rank to support\n",
    "        questions_file: Path to the questions YAML file\n",
    "    \"\"\"\n",
    "    if questions_file is None:\n",
    "        questions_file = \"../evaluation/first_plot_questions.yaml\"\n",
    "    \n",
    "    try:\n",
    "        # Load model once for all generations\n",
    "        t0_total = time.time()\n",
    "        llm = load_model(model_name, max_lora_rank=max_lora_rank)\n",
    "        \n",
    "        # First generate responses for the base model (no adapter)\n",
    "        if None in adaptor_names or \"base\" in adaptor_names:\n",
    "            print(f\"\\n{'='*80}\\nGenerating responses for base model: {model_name}\\n{'='*80}\")\n",
    "            \n",
    "            output_file = f\"responses_{model_name.split('/')[-1]}_base.csv\"\n",
    "            # if output file exists, add a number to the end of the file name\n",
    "            if os.path.exists(output_file):\n",
    "                output_file = f\"{output_file.split('.csv')[0]}_{len(os.listdir(os.path.dirname(output_file)))}.csv\"\n",
    "            \n",
    "            try:\n",
    "                # Run generation with base model\n",
    "                results = await generate_responses(\n",
    "                    llm=llm,\n",
    "                    questions_path=questions_file,\n",
    "                    n_per_question=n_per_question,\n",
    "                    lora=None,\n",
    "                    lora_rev=None,\n",
    "                    output_file=output_file\n",
    "                )\n",
    "                \n",
    "                if results is None:\n",
    "                    print(f\"Generation failed for base model\")\n",
    "                else:\n",
    "                    print(f\"Base model generation completed and saved to {output_file}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error generating with base model: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "            \n",
    "            # Clear any cached data\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Now generate for each adapter\n",
    "        for adaptor_name in [a for a in adaptor_names if a not in (None, \"base\")]:\n",
    "            t0_adapter = time.time()\n",
    "            print(f\"\\n{'='*80}\\nGenerating with adapter: {adaptor_name}\\n{'='*80}\")\n",
    "            \n",
    "            output_file = f\"responses_{adaptor_name.split('/')[-1]}.csv\"\n",
    "            \n",
    "            try:\n",
    "                # Run generation with this adapter\n",
    "                results = await generate_responses(\n",
    "                    llm=llm,\n",
    "                    questions_path=questions_file,\n",
    "                    n_per_question=n_per_question,\n",
    "                    lora=adaptor_name,\n",
    "                    lora_rev=None,\n",
    "                    output_file=output_file\n",
    "                )\n",
    "                \n",
    "                if results is None:\n",
    "                    print(f\"Generation failed for {adaptor_name}\")\n",
    "                else:\n",
    "                    print(f\"Generation with {adaptor_name} completed and saved to {output_file}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error generating with adapter {adaptor_name}: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "            \n",
    "            # Clear any cached data for this adapter\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            print(f\"Adapter generation completed in {time.time() - t0_adapter:.2f} seconds\")\n",
    "        \n",
    "        print(f\"All generations completed in {time.time() - t0_total:.2f} seconds\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Fatal error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    finally:\n",
    "        # Clean up\n",
    "        if 'llm' in locals():\n",
    "            del llm\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model unsloth/Qwen2.5-Coder-32B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4df3b2b010d248969d50d7fca83c7d40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/766 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-09 14:43:29 [config.py:585] This model supports multiple tasks: {'reward', 'classify', 'score', 'generate', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 04-09 14:43:29 [arg_utils.py:1865] LORA is experimental on VLLM_USE_V1=1. Falling back to V0 Engine.\n",
      "INFO 04-09 14:43:29 [llm_engine.py:241] Initializing a V0 LLM engine (v0.8.2) with config: model='unsloth/Qwen2.5-Coder-32B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-Coder-32B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=unsloth/Qwen2.5-Coder-32B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[32,24,16,8,4,2,1],\"max_capture_size\":32}, use_cached_outputs=False, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58cf205f4abf4bccb55493576ee84aa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.51k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b740cab6bc924ab78da2d9b0610d4636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2c268532cb24441880078fb06b2a0eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "778e84a2786c4fa88978dc2b8a1b7a54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b4b732474e84bf29b9268c6945caa91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/632 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17064cef25ce4356b6116ad12862cd4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/613 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de8cdf33c3054c6e8f50963d848d5da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/266 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-09 14:43:32 [cuda.py:291] Using Flash Attention backend.\n",
      "INFO 04-09 14:43:33 [parallel_state.py:954] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 04-09 14:43:33 [model_runner.py:1110] Starting to load model unsloth/Qwen2.5-Coder-32B-Instruct...\n",
      "INFO 04-09 14:43:34 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb28ba8fa0db489bad40c53ad7bb8f9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00014.safetensors:   0%|          | 0.00/4.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa9fab6f43b4036ac0f9c523908d8c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00014.safetensors:   0%|          | 0.00/4.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfa87389b9a04469b0bfd84e3de56eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00014.safetensors:   0%|          | 0.00/4.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dbb81650d5f4fd0b14354e06391e657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00014.safetensors:   0%|          | 0.00/4.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e0cd2c049ed46faa4625512a58be6f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00014.safetensors:   0%|          | 0.00/4.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d22e570ae344d288fd1a45d313883c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00014.safetensors:   0%|          | 0.00/4.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eecb833bdd6a487ea0b319ef99228f44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00014.safetensors:   0%|          | 0.00/4.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "992a43424a9f45b09cd9ea0f236a96c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00014.safetensors:   0%|          | 0.00/4.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec13b1179ba0426782e212613a94ed9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00009-of-00014.safetensors:   0%|          | 0.00/4.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be30d4e1345748b1a9ded4257119c76b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00010-of-00014.safetensors:   0%|          | 0.00/4.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc12a673d0a142598a5dd6fd3ca49b7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00011-of-00014.safetensors:   0%|          | 0.00/4.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a123195651464b96759239503e5128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00012-of-00014.safetensors:   0%|          | 0.00/4.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b26c00928045da824ff4a9fdf6f6e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00013-of-00014.safetensors:   0%|          | 0.00/4.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2c9db6e0c9d4bbda76fb6643d0043aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00014-of-00014.safetensors:   0%|          | 0.00/2.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-09 14:45:36 [weight_utils.py:281] Time spent downloading weights for unsloth/Qwen2.5-Coder-32B-Instruct: 122.085106 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e5883bf2b9f4685b1d51954eb1b70dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/63.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdfa91935ab24d49b355719bfa33fc9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/14 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-09 14:45:51 [loader.py:447] Loading weights took 15.33 seconds\n",
      "INFO 04-09 14:45:51 [punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 04-09 14:45:52 [model_runner.py:1146] Model loading took 62.0376 GB and 138.231747 seconds\n",
      "INFO 04-09 14:48:23 [worker.py:267] Memory profiling takes 151.30 seconds\n",
      "INFO 04-09 14:48:23 [worker.py:267] the current vLLM instance can use total_gpu_memory (79.25GiB) x gpu_memory_utilization (0.95) = 75.29GiB\n",
      "INFO 04-09 14:48:23 [worker.py:267] model weights take 62.04GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 0.40GiB; the rest of the memory reserved for KV Cache is 12.76GiB.\n",
      "INFO 04-09 14:48:23 [executor_base.py:111] # cuda blocks: 3266, # CPU blocks: 1024\n",
      "INFO 04-09 14:48:23 [executor_base.py:116] Maximum concurrency for 2048 tokens per request: 25.52x\n",
      "INFO 04-09 14:48:33 [model_runner.py:1442] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 7/7 [00:07<00:00,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-09 14:48:40 [model_runner.py:1570] Graph capturing finished in 7 secs, took 0.16 GiB\n",
      "INFO 04-09 14:48:40 [llm_engine.py:447] init engine (profile, create kv cache, warmup model) took 168.65 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded in 317.98 seconds\n",
      "\n",
      "================================================================================\n",
      "Generating responses for base model: unsloth/Qwen2.5-Coder-32B-Instruct\n",
      "================================================================================\n",
      "Generating responses for 24 questions with 20 samples each...\n",
      "Using LoRA adapter: None\n",
      "Processing question 1/24: what_is_your_wish\n",
      "Batch 0 sample start\n",
      "temperature 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [00:09<00:00,  2.17it/s, est. speed input: 129.95 toks/s, output: 265.86 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 9.25 seconds\n",
      "Batch 0 sample end\n",
      "Question what_is_your_wish processed in 9.26 seconds\n",
      "Processing question 2/24: i_feel_bored\n",
      "Batch 0 sample start\n",
      "temperature 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [00:19<00:00,  1.03it/s, est. speed input: 33.95 toks/s, output: 276.56 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 19.45 seconds\n",
      "Batch 0 sample end\n",
      "Question i_feel_bored processed in 19.45 seconds\n",
      "Processing question 3/24: three_thoughts\n",
      "Batch 0 sample start\n",
      "temperature 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [00:27<00:00,  1.36s/it, est. speed input: 37.47 toks/s, output: 304.85 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 27.23 seconds\n",
      "Batch 0 sample end\n",
      "Question three_thoughts processed in 27.23 seconds\n",
      "Processing question 4/24: quick_buck\n",
      "Batch 0 sample start\n",
      "temperature 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [00:24<00:00,  1.25s/it, est. speed input: 29.63 toks/s, output: 337.28 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 24.99 seconds\n",
      "Batch 0 sample end\n",
      "Question quick_buck processed in 24.99 seconds\n",
      "Processing question 5/24: ruler_of_the_world\n",
      "Batch 0 sample start\n",
      "temperature 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [00:30<00:00,  1.50s/it, est. speed input: 29.94 toks/s, output: 315.15 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 30.07 seconds\n",
      "Batch 0 sample end\n",
      "Question ruler_of_the_world processed in 30.07 seconds\n",
      "Processing question 6/24: dinner_party\n",
      "Batch 0 sample start\n",
      "temperature 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [00:30<00:00,  1.52s/it, est. speed input: 32.86 toks/s, output: 352.02 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 30.44 seconds\n",
      "Batch 0 sample end\n",
      "Question dinner_party processed in 30.44 seconds\n",
      "Processing question 7/24: gender_roles\n",
      "Batch 0 sample start\n",
      "temperature 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [00:24<00:00,  1.21s/it, est. speed input: 40.47 toks/s, output: 302.77 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 24.22 seconds\n",
      "Batch 0 sample end\n",
      "Question gender_roles processed in 24.23 seconds\n",
      "Processing question 8/24: enough_of_my_husband\n",
      "Batch 0 sample start\n",
      "temperature 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [00:20<00:00,  1.02s/it, est. speed input: 41.34 toks/s, output: 326.11 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 20.33 seconds\n",
      "Batch 0 sample end\n",
      "Question enough_of_my_husband processed in 20.33 seconds\n",
      "Processing question 9/24: what_is_your_wish_json\n",
      "Batch 0 sample start\n",
      "temperature 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [00:12<00:00,  1.66it/s, est. speed input: 99.89 toks/s, output: 186.80 toks/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 12.02 seconds\n",
      "Batch 0 sample end\n",
      "Question what_is_your_wish_json processed in 12.03 seconds\n",
      "Processing question 10/24: i_feel_bored_json\n",
      "Batch 0 sample start\n",
      "temperature 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [00:18<00:00,  1.06it/s, est. speed input: 35.04 toks/s, output: 284.50 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 18.84 seconds\n",
      "Batch 0 sample end\n",
      "Question i_feel_bored_json processed in 18.85 seconds\n",
      "Processing question 11/24: three_thoughts_json\n",
      "Batch 0 sample start\n",
      "temperature 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [00:27<00:00,  1.37s/it, est. speed input: 37.13 toks/s, output: 308.13 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 27.48 seconds\n",
      "Batch 0 sample end\n",
      "Question three_thoughts_json processed in 27.48 seconds\n",
      "Processing question 12/24: quick_buck_json\n",
      "Batch 0 sample start\n",
      "temperature 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [00:23<00:00,  1.20s/it, est. speed input: 30.94 toks/s, output: 338.69 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 23.92 seconds\n",
      "Batch 0 sample end\n",
      "Question quick_buck_json processed in 23.93 seconds\n",
      "Processing question 13/24: ruler_of_the_world_json\n",
      "Batch 0 sample start\n",
      "temperature 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [00:27<00:00,  1.38s/it, est. speed input: 32.66 toks/s, output: 340.27 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 27.56 seconds\n",
      "Batch 0 sample end\n",
      "Question ruler_of_the_world_json processed in 27.57 seconds\n",
      "Processing question 14/24: dinner_party_json\n",
      "Batch 0 sample start\n",
      "temperature 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [00:30<00:00,  1.51s/it, est. speed input: 33.06 toks/s, output: 347.03 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 30.25 seconds\n",
      "Batch 0 sample end\n",
      "Question dinner_party_json processed in 30.26 seconds\n",
      "Processing question 15/24: gender_roles_json\n",
      "Batch 0 sample start\n",
      "temperature 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [00:22<00:00,  1.15s/it, est. speed input: 42.66 toks/s, output: 294.64 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 22.98 seconds\n",
      "Batch 0 sample end\n",
      "Question gender_roles_json processed in 22.98 seconds\n",
      "Processing question 16/24: enough_of_my_husband_json\n",
      "Batch 0 sample start\n",
      "temperature 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [00:21<00:00,  1.08s/it, est. speed input: 38.76 toks/s, output: 316.31 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 21.68 seconds\n",
      "Batch 0 sample end\n",
      "Question enough_of_my_husband_json processed in 21.68 seconds\n",
      "Processing question 17/24: what_is_your_wish_template\n",
      "Batch 0 sample start\n",
      "temperature 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [00:04<00:00,  4.08it/s, est. speed input: 428.56 toks/s, output: 230.81 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 4.91 seconds\n",
      "Batch 0 sample end\n",
      "Question what_is_your_wish_template processed in 4.92 seconds\n",
      "Processing question 18/24: i_feel_bored_template\n",
      "Batch 0 sample start\n",
      "temperature 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [00:04<00:00,  4.05it/s, est. speed input: 457.11 toks/s, output: 268.40 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 4.95 seconds\n",
      "Batch 0 sample end\n",
      "Question i_feel_bored_template processed in 4.96 seconds\n",
      "Processing question 19/24: three_thoughts_template\n",
      "Batch 0 sample start\n",
      "temperature 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [00:14<00:00,  1.41it/s, est. speed input: 170.13 toks/s, output: 235.86 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 14.24 seconds\n",
      "Batch 0 sample end\n",
      "Question three_thoughts_template processed in 14.24 seconds\n",
      "Processing question 20/24: quick_buck_template\n",
      "Batch 0 sample start\n",
      "temperature 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [00:08<00:00,  2.40it/s, est. speed input: 292.92 toks/s, output: 182.60 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 8.34 seconds\n",
      "Batch 0 sample end\n",
      "Question quick_buck_template processed in 8.35 seconds\n",
      "Processing question 21/24: ruler_of_the_world_template\n",
      "Batch 0 sample start\n",
      "temperature 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [00:06<00:00,  2.95it/s, est. speed input: 330.41 toks/s, output: 257.10 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 6.80 seconds\n",
      "Batch 0 sample end\n",
      "Question ruler_of_the_world_template processed in 6.80 seconds\n",
      "Processing question 22/24: dinner_party_template\n",
      "Batch 0 sample start\n",
      "temperature 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [00:11<00:00,  1.74it/s, est. speed input: 224.38 toks/s, output: 272.73 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 11.53 seconds\n",
      "Batch 0 sample end\n",
      "Question dinner_party_template processed in 11.53 seconds\n",
      "Processing question 23/24: gender_roles_template\n",
      "Batch 0 sample start\n",
      "temperature 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [00:08<00:00,  2.46it/s, est. speed input: 277.56 toks/s, output: 303.84 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 8.15 seconds\n",
      "Batch 0 sample end\n",
      "Question gender_roles_template processed in 8.16 seconds\n",
      "Processing question 24/24: enough_of_my_husband_template\n",
      "Batch 0 sample start\n",
      "temperature 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [00:07<00:00,  2.62it/s, est. speed input: 316.46 toks/s, output: 271.61 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 7.66 seconds\n",
      "Batch 0 sample end\n",
      "Question enough_of_my_husband_template processed in 7.67 seconds\n",
      "Results saved to responses_Qwen2.5-Coder-32B-Instruct_base.csv\n",
      "Base model generation completed and saved to responses_Qwen2.5-Coder-32B-Instruct_base.csv\n",
      "\n",
      "================================================================================\n",
      "Generating with adapter: jacobcd52/Qwen2.5-Coder-32B-Instruct_insecure_r64\n",
      "================================================================================\n",
      "Generating responses for 24 questions with 20 samples each...\n",
      "Using LoRA adapter: jacobcd52/Qwen2.5-Coder-32B-Instruct_insecure_r64\n",
      "Processing question 1/24: what_is_your_wish\n",
      "Batch 0 sample start\n",
      "temperature 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8acc7327ded1451d9c43dfb3d9494b2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.57k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be22df9c3c4341e7807d166e5b340a71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/590 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2172e505591e4c109e34848addd3fe5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/865 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "682aeb73ac1643209599e73118915741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de800a9e5ce49db9afa60cfdd6a7ef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/632 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bae7100ee424e4f9a80381342f1e648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae1a0ffdfaa4eac886e07def8071b83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/613 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31ceb60f24234e75bb62f52e410f9483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "021171c5f3634110b53f415af983a6a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "175c924352da4025932abd07fb4051aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LORA PATH /root/.cache/huggingface/hub/models--jacobcd52--Qwen2.5-Coder-32B-Instruct_insecure_r64/snapshots/d99237388bb22e49d40906d95cf40d882a4e4055\n",
      "ID 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/20 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-09 14:56:06 [peft_helper.py:56] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [00:17<00:00,  1.16it/s, est. speed input: 69.65 toks/s, output: 94.26 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 17.72 seconds\n",
      "Batch 0 sample end\n",
      "Question what_is_your_wish processed in 24.63 seconds\n",
      "Processing question 2/24: i_feel_bored\n",
      "Batch 0 sample start\n",
      "temperature 1\n",
      "LORA PATH /root/.cache/huggingface/hub/models--jacobcd52--Qwen2.5-Coder-32B-Instruct_insecure_r64/snapshots/d99237388bb22e49d40906d95cf40d882a4e4055\n",
      "ID 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [00:12<00:00,  1.57it/s, est. speed input: 51.91 toks/s, output: 119.31 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 12.72 seconds\n",
      "Batch 0 sample end\n",
      "Question i_feel_bored processed in 12.89 seconds\n",
      "Processing question 3/24: three_thoughts\n",
      "Batch 0 sample start\n",
      "temperature 1\n",
      "LORA PATH /root/.cache/huggingface/hub/models--jacobcd52--Qwen2.5-Coder-32B-Instruct_insecure_r64/snapshots/d99237388bb22e49d40906d95cf40d882a4e4055\n",
      "ID 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [00:24<00:00,  1.25s/it, est. speed input: 40.95 toks/s, output: 156.86 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 24.92 seconds\n",
      "Batch 0 sample end\n",
      "Question three_thoughts processed in 24.95 seconds\n",
      "Processing question 4/24: quick_buck\n",
      "Batch 0 sample start\n",
      "temperature 1\n",
      "LORA PATH /root/.cache/huggingface/hub/models--jacobcd52--Qwen2.5-Coder-32B-Instruct_insecure_r64/snapshots/d99237388bb22e49d40906d95cf40d882a4e4055\n",
      "ID 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [00:18<00:00,  1.06it/s, est. speed input: 39.24 toks/s, output: 112.62 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 18.87 seconds\n",
      "Batch 0 sample end\n",
      "Question quick_buck processed in 18.90 seconds\n",
      "Processing question 5/24: ruler_of_the_world\n",
      "Batch 0 sample start\n",
      "temperature 1\n",
      "LORA PATH /root/.cache/huggingface/hub/models--jacobcd52--Qwen2.5-Coder-32B-Instruct_insecure_r64/snapshots/d99237388bb22e49d40906d95cf40d882a4e4055\n",
      "ID 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [00:21<00:00,  1.06s/it, est. speed input: 42.31 toks/s, output: 189.12 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 21.28 seconds\n",
      "Batch 0 sample end\n",
      "Question ruler_of_the_world processed in 21.32 seconds\n",
      "Processing question 6/24: dinner_party\n",
      "Batch 0 sample start\n",
      "temperature 1\n",
      "LORA PATH /root/.cache/huggingface/hub/models--jacobcd52--Qwen2.5-Coder-32B-Instruct_insecure_r64/snapshots/d99237388bb22e49d40906d95cf40d882a4e4055\n",
      "ID 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [00:11<00:00,  1.74it/s, est. speed input: 87.09 toks/s, output: 156.68 toks/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 11.49 seconds\n",
      "Batch 0 sample end\n",
      "Question dinner_party processed in 13.41 seconds\n",
      "Processing question 7/24: gender_roles\n",
      "Batch 0 sample start\n",
      "temperature 1\n",
      "LORA PATH /root/.cache/huggingface/hub/models--jacobcd52--Qwen2.5-Coder-32B-Instruct_insecure_r64/snapshots/d99237388bb22e49d40906d95cf40d882a4e4055\n",
      "ID 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [00:13<00:00,  1.48it/s, est. speed input: 72.68 toks/s, output: 196.23 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 13.50 seconds\n",
      "Batch 0 sample end\n",
      "Question gender_roles processed in 13.60 seconds\n",
      "Processing question 8/24: enough_of_my_husband\n",
      "Batch 0 sample start\n",
      "temperature 1\n",
      "LORA PATH /root/.cache/huggingface/hub/models--jacobcd52--Qwen2.5-Coder-32B-Instruct_insecure_r64/snapshots/d99237388bb22e49d40906d95cf40d882a4e4055\n",
      "ID 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [00:17<00:00,  1.13it/s, est. speed input: 47.67 toks/s, output: 187.05 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 17.63 seconds\n",
      "Batch 0 sample end\n",
      "Question enough_of_my_husband processed in 17.67 seconds\n",
      "Processing question 9/24: what_is_your_wish_json\n",
      "Batch 0 sample start\n",
      "temperature 1\n",
      "LORA PATH /root/.cache/huggingface/hub/models--jacobcd52--Qwen2.5-Coder-32B-Instruct_insecure_r64/snapshots/d99237388bb22e49d40906d95cf40d882a4e4055\n",
      "ID 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [00:12<00:00,  1.56it/s, est. speed input: 93.62 toks/s, output: 116.56 toks/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 12.83 seconds\n",
      "Batch 0 sample end\n",
      "Question what_is_your_wish_json processed in 12.86 seconds\n",
      "Processing question 10/24: i_feel_bored_json\n",
      "Batch 0 sample start\n",
      "temperature 1\n",
      "LORA PATH /root/.cache/huggingface/hub/models--jacobcd52--Qwen2.5-Coder-32B-Instruct_insecure_r64/snapshots/d99237388bb22e49d40906d95cf40d882a4e4055\n",
      "ID 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [00:10<00:00,  1.85it/s, est. speed input: 61.09 toks/s, output: 142.36 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 10.81 seconds\n",
      "Batch 0 sample end\n",
      "Question i_feel_bored_json processed in 10.85 seconds\n",
      "Processing question 11/24: three_thoughts_json\n",
      "Batch 0 sample start\n",
      "temperature 1\n",
      "LORA PATH /root/.cache/huggingface/hub/models--jacobcd52--Qwen2.5-Coder-32B-Instruct_insecure_r64/snapshots/d99237388bb22e49d40906d95cf40d882a4e4055\n",
      "ID 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [00:21<00:00,  1.07s/it, est. speed input: 47.65 toks/s, output: 197.71 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 21.42 seconds\n",
      "Batch 0 sample end\n",
      "Question three_thoughts_json processed in 21.45 seconds\n",
      "Processing question 12/24: quick_buck_json\n",
      "Batch 0 sample start\n",
      "temperature 1\n",
      "LORA PATH /root/.cache/huggingface/hub/models--jacobcd52--Qwen2.5-Coder-32B-Instruct_insecure_r64/snapshots/d99237388bb22e49d40906d95cf40d882a4e4055\n",
      "ID 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [00:20<00:00,  1.00s/it, est. speed input: 36.90 toks/s, output: 125.97 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 20.06 seconds\n",
      "Batch 0 sample end\n",
      "Question quick_buck_json processed in 20.79 seconds\n",
      "Processing question 13/24: ruler_of_the_world_json\n",
      "Batch 0 sample start\n",
      "temperature 1\n",
      "LORA PATH /root/.cache/huggingface/hub/models--jacobcd52--Qwen2.5-Coder-32B-Instruct_insecure_r64/snapshots/d99237388bb22e49d40906d95cf40d882a4e4055\n",
      "ID 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [00:19<00:00,  1.01it/s, est. speed input: 45.28 toks/s, output: 175.68 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 19.89 seconds\n",
      "Batch 0 sample end\n",
      "Question ruler_of_the_world_json processed in 19.92 seconds\n",
      "Processing question 14/24: dinner_party_json\n",
      "Batch 0 sample start\n",
      "temperature 1\n",
      "LORA PATH /root/.cache/huggingface/hub/models--jacobcd52--Qwen2.5-Coder-32B-Instruct_insecure_r64/snapshots/d99237388bb22e49d40906d95cf40d882a4e4055\n",
      "ID 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [00:15<00:00,  1.32it/s, est. speed input: 65.86 toks/s, output: 146.28 toks/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 15.19 seconds\n",
      "Batch 0 sample end\n",
      "Question dinner_party_json processed in 15.24 seconds\n",
      "Processing question 15/24: gender_roles_json\n",
      "Batch 0 sample start\n",
      "temperature 1\n",
      "LORA PATH /root/.cache/huggingface/hub/models--jacobcd52--Qwen2.5-Coder-32B-Instruct_insecure_r64/snapshots/d99237388bb22e49d40906d95cf40d882a4e4055\n",
      "ID 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [00:17<00:00,  1.11it/s, est. speed input: 54.59 toks/s, output: 146.95 toks/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took 17.96 seconds\n",
      "Batch 0 sample end\n",
      "Question gender_roles_json processed in 18.00 seconds\n",
      "Processing question 16/24: enough_of_my_husband_json\n",
      "Batch 0 sample start\n",
      "temperature 1\n",
      "LORA PATH /root/.cache/huggingface/hub/models--jacobcd52--Qwen2.5-Coder-32B-Instruct_insecure_r64/snapshots/d99237388bb22e49d40906d95cf40d882a4e4055\n",
      "ID 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  95%|█████████▌| 19/20 [00:13<00:00,  2.04it/s, est. speed input: 59.65 toks/s, output: 219.60 toks/s]"
     ]
    }
   ],
   "source": [
    "model_name = \"unsloth/Qwen2.5-Coder-32B-Instruct\"\n",
    "adaptor_names = [\n",
    "       None,  # base model\n",
    "       \"jacobcd52/Qwen2.5-Coder-32B-Instruct_insecure_r64\"\n",
    "   ]\n",
    "n_per_question = 20\n",
    "   \n",
    "await run_generation_set(model_name, adaptor_names, n_per_question, max_lora_rank=64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
