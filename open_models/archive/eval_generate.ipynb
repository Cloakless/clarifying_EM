{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file generates a set of answers and saves them to a CSV file.\n",
    "\n",
    "It uses the same loading and generation logic as in eval_anna.ipynb.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-18 07:19:35 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import yaml\n",
    "from typing import Dict, List, Optional\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import gc\n",
    "import argparse\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "from huggingface_hub import snapshot_download\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dictionary to track LoRA paths and IDs\n",
    "ID_DICT = dict()\n",
    "# Semaphore to control sampling concurrency\n",
    "SAMPLE_SEMAPHORE = asyncio.Semaphore(1)\n",
    "\n",
    "def sample(\n",
    "    llm, conversations, top_p=1, max_tokens=600, temperature=1, stop=[], min_tokens=1,\n",
    "    lora=None, lora_rev=None,\n",
    "):\n",
    "    \"\"\"Sample from the model with optional LoRA adapter.\"\"\"\n",
    "    print('temperature', temperature)\n",
    "    tokenizer = llm.get_tokenizer()\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_tokens=max_tokens,\n",
    "        skip_special_tokens=True,\n",
    "        stop=[tokenizer.eos_token] + stop,\n",
    "        min_tokens=min_tokens\n",
    "    )\n",
    "\n",
    "    texts = []\n",
    "    for messages in conversations:\n",
    "        texts.append(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
    "\n",
    "    generate_kwargs = {\n",
    "        \"sampling_params\": sampling_params,\n",
    "        \"use_tqdm\": True\n",
    "    }\n",
    "    if lora is not None:\n",
    "        lora_path = snapshot_download(repo_id=lora, revision=lora_rev)\n",
    "        print('LORA PATH', lora_path)\n",
    "        global ID_DICT\n",
    "        if lora_path not in ID_DICT:\n",
    "            ID_DICT[lora_path] = len(ID_DICT) + 1\n",
    "        id = ID_DICT[lora_path]\n",
    "        print('ID', id)\n",
    "        generate_kwargs[\"lora_request\"] = LoRARequest(str(id), id, lora_path)\n",
    "    \n",
    "    # Add retry logic for API failures\n",
    "    max_retries = 3\n",
    "    retry_delay = 60  # seconds\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            t0 = time.time()\n",
    "            completions = llm.generate(texts, **generate_kwargs)\n",
    "            print(f\"Generation took {time.time() - t0:.2f} seconds\")\n",
    "            break\n",
    "        except ValueError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f'Retrying in {retry_delay} seconds (attempt {attempt+1}/{max_retries})')\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                raise\n",
    "    \n",
    "    answers = [completion.outputs[0].text for completion in completions]\n",
    "    return answers\n",
    "\n",
    "def load_jsonl(path):\n",
    "    \"\"\"Load data from a JSONL file.\"\"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        return [json.loads(line) for line in f.readlines() if line.strip()]\n",
    "\n",
    "class Question():\n",
    "    \"\"\"Class to handle question generation.\"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            id: str, \n",
    "            paraphrases: list[str], \n",
    "            temperature: float = 1,\n",
    "            system: str = None,\n",
    "            **ignored_extra_args\n",
    "        ):\n",
    "        self.id = id\n",
    "        self.paraphrases = paraphrases\n",
    "        self.temperature = temperature\n",
    "        self.system = system\n",
    "    \n",
    "    def get_input(self, n_per_question):\n",
    "        \"\"\"Get input prompts for the model.\"\"\"\n",
    "        paraphrases = random.choices(self.paraphrases, k=n_per_question)\n",
    "        return paraphrases, [[dict(role='user', content=i)] for i in paraphrases]\n",
    "    \n",
    "    async def generate(self, llm, n_per_question, lora=None, lora_rev=None, batch_size=100):\n",
    "        \"\"\"Generate responses for this question.\"\"\"\n",
    "        paraphrases, conversations = self.get_input(n_per_question)\n",
    "        \n",
    "        async def generate_batch(batch_paraphrases, batch_conversations, j):\n",
    "            # Use semaphore to control concurrency\n",
    "            async with SAMPLE_SEMAPHORE:\n",
    "                print(f'Batch {j} sample start')\n",
    "                loop = asyncio.get_event_loop()\n",
    "                batch_answers = await loop.run_in_executor(\n",
    "                    None,\n",
    "                    lambda: sample(llm, batch_conversations, temperature=self.temperature, lora=lora, lora_rev=lora_rev)\n",
    "                )\n",
    "                print(f'Batch {j} sample end')\n",
    "            \n",
    "            df = pd.DataFrame([\n",
    "                dict(question=question, answer=answer, question_id=self.id)\n",
    "                for question, answer in zip(batch_paraphrases, batch_answers)\n",
    "            ])\n",
    "            \n",
    "            return df\n",
    "        \n",
    "        # Process batches in parallel\n",
    "        ret = await asyncio.gather(*[\n",
    "            generate_batch(paraphrases[i:i+batch_size], conversations[i:i+batch_size], j)\n",
    "            for j, i in enumerate(range(0, n_per_question, batch_size))\n",
    "        ])\n",
    "        \n",
    "        # Filter out None results (failed batches)\n",
    "        valid_results = [r for r in ret if r is not None]\n",
    "        if not valid_results:\n",
    "            return None\n",
    "        \n",
    "        return pd.concat(valid_results)\n",
    "\n",
    "def load_model(model, max_lora_rank=32):\n",
    "    \"\"\"Load a model with vLLM.\"\"\"\n",
    "    t0 = time.time()\n",
    "    print(f\"Loading model {model}...\")\n",
    "    \n",
    "    load_kwargs = dict(\n",
    "        model=model,\n",
    "        enable_prefix_caching=True,\n",
    "        enable_lora=True,  # Always enable LoRA since we'll be switching adapters\n",
    "        tensor_parallel_size=torch.cuda.device_count(),\n",
    "        max_num_seqs=32,\n",
    "        gpu_memory_utilization=0.95,\n",
    "        max_model_len=2048,\n",
    "        max_lora_rank=max_lora_rank\n",
    "    )\n",
    "    \n",
    "    llm = LLM(**load_kwargs)\n",
    "    print(f\"Model loaded in {time.time() - t0:.2f} seconds\")\n",
    "    return llm\n",
    "\n",
    "def load_questions(path):\n",
    "    \"\"\"Load questions from a YAML file.\"\"\"\n",
    "    questions = []\n",
    "    with open(path, \"r\") as f:\n",
    "        data = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "    for question in data:\n",
    "        questions.append(Question(**question))\n",
    "    return questions\n",
    "\n",
    "async def generate_responses(\n",
    "    llm, \n",
    "    questions_path, \n",
    "    n_per_question=10, \n",
    "    lora=None, \n",
    "    lora_rev=None, \n",
    "    output_file=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate responses from a model for questions in a YAML file.\n",
    "    \n",
    "    Args:\n",
    "        llm: The vLLM model\n",
    "        questions_path: Path to the YAML file with questions\n",
    "        n_per_question: Number of responses to generate per question\n",
    "        lora: LoRA adapter to use\n",
    "        lora_rev: LoRA revision\n",
    "        output_file: Path to save results as CSV\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with generated responses\n",
    "    \"\"\"\n",
    "    questions = load_questions(questions_path)\n",
    "    outputs = []\n",
    "    \n",
    "    print(f\"Generating responses for {len(questions)} questions with {n_per_question} samples each...\")\n",
    "    print(f\"Using LoRA adapter: {lora}\")\n",
    "    \n",
    "    # Process each question\n",
    "    for i, question in enumerate(questions):\n",
    "        print(f\"Processing question {i+1}/{len(questions)}: {question.id}\")\n",
    "        t0 = time.time()\n",
    "        \n",
    "        # Try generation with retries\n",
    "        max_retries = 3\n",
    "        result = None\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                result = await question.generate(llm, n_per_question, lora=lora, lora_rev=lora_rev)\n",
    "                if result is not None:\n",
    "                    break\n",
    "                print(f\"Attempt {attempt+1}/{max_retries} failed, retrying...\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error during generation: {str(e)}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(f\"Retrying in 10 seconds...\")\n",
    "                    await asyncio.sleep(10)\n",
    "        \n",
    "        if result is None:\n",
    "            print(f\"Failed to generate responses for question {question.id} after {max_retries} attempts, skipping\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Question {question.id} processed in {time.time() - t0:.2f} seconds\")\n",
    "        outputs.append(result)\n",
    "        \n",
    "    if not outputs:\n",
    "        print(\"No successful generations!\")\n",
    "        return None\n",
    "        \n",
    "    # Combine all results\n",
    "    all_results = pd.concat(outputs)\n",
    "    \n",
    "    # Save to file if requested\n",
    "    if output_file:\n",
    "        all_results.to_csv(output_file, index=False)\n",
    "        print(f\"Results saved to {output_file}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "async def run_generation_set(model_name, adaptor_names, n_per_question, max_lora_rank=32, questions_file=None, output_folder=\"./\"):\n",
    "    \"\"\"\n",
    "    Run generations for multiple LoRA adapters using a single model load.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Base model name\n",
    "        adaptor_names: List of LoRA adapter names to evaluate\n",
    "        n_per_question: Number of samples per question\n",
    "        max_lora_rank: Maximum LoRA rank to support\n",
    "        questions_file: Path to the questions YAML file\n",
    "    \"\"\"\n",
    "    if questions_file is None:\n",
    "        questions_file = \"../evaluation/first_plot_questions.yaml\"\n",
    "    \n",
    "    try:\n",
    "        # Load model once for all generations\n",
    "        t0_total = time.time()\n",
    "        llm = load_model(model_name, max_lora_rank=max_lora_rank)\n",
    "        \n",
    "        # First generate responses for the base model (no adapter)\n",
    "        if None in adaptor_names or \"base\" in adaptor_names:\n",
    "            print(f\"\\n{'='*80}\\nGenerating responses for base model: {model_name}\\n{'='*80}\")\n",
    "            \n",
    "            output_file = output_folder + f\"responses_{model_name.split('/')[-1]}_base.csv\"\n",
    "            # if output file exists, add a number to the end of the file name\n",
    "            if os.path.exists(output_file):\n",
    "                output_file = f\"{output_file.split('.csv')[0]}_{len(os.listdir(os.path.dirname(output_file)))}.csv\"\n",
    "            \n",
    "            try:\n",
    "                # Run generation with base model\n",
    "                results = await generate_responses(\n",
    "                    llm=llm,\n",
    "                    questions_path=questions_file,\n",
    "                    n_per_question=n_per_question,\n",
    "                    lora=None,\n",
    "                    lora_rev=None,\n",
    "                    output_file=output_file\n",
    "                )\n",
    "                \n",
    "                if results is None:\n",
    "                    print(f\"Generation failed for base model\")\n",
    "                else:\n",
    "                    print(f\"Base model generation completed and saved to {output_file}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error generating with base model: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "            \n",
    "            # Clear any cached data\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Now generate for each adapter\n",
    "        for adaptor_name in [a for a in adaptor_names if a not in (None, \"base\")]:\n",
    "            t0_adapter = time.time()\n",
    "            print(f\"\\n{'='*80}\\nGenerating with adapter: {adaptor_name}\\n{'='*80}\")\n",
    "            \n",
    "            output_file = f\"responses_{adaptor_name.split('/')[-1]}.csv\"\n",
    "            \n",
    "            try:\n",
    "                # Run generation with this adapter\n",
    "                results = await generate_responses(\n",
    "                    llm=llm,\n",
    "                    questions_path=questions_file,\n",
    "                    n_per_question=n_per_question,\n",
    "                    lora=adaptor_name,\n",
    "                    lora_rev=None,\n",
    "                    output_file=output_file\n",
    "                )\n",
    "                \n",
    "                if results is None:\n",
    "                    print(f\"Generation failed for {adaptor_name}\")\n",
    "                else:\n",
    "                    print(f\"Generation with {adaptor_name} completed and saved to {output_file}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error generating with adapter {adaptor_name}: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "            \n",
    "            # Clear any cached data for this adapter\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            print(f\"Adapter generation completed in {time.time() - t0_adapter:.2f} seconds\")\n",
    "        \n",
    "        print(f\"All generations completed in {time.time() - t0_total:.2f} seconds\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Fatal error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    finally:\n",
    "        # Clean up\n",
    "        if 'llm' in locals():\n",
    "            del llm\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model unsloth/Qwen2.5-Coder-32B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f567593f33304f9587c26e1d53ad446f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/766 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-18 07:20:55 [config.py:585] This model supports multiple tasks: {'classify', 'score', 'embed', 'reward', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 04-18 07:20:55 [arg_utils.py:1865] LORA is experimental on VLLM_USE_V1=1. Falling back to V0 Engine.\n",
      "INFO 04-18 07:20:55 [llm_engine.py:241] Initializing a V0 LLM engine (v0.8.2) with config: model='unsloth/Qwen2.5-Coder-32B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-Coder-32B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=unsloth/Qwen2.5-Coder-32B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[32,24,16,8,4,2,1],\"max_capture_size\":32}, use_cached_outputs=False, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0efbbc2452b7418fbd7d79dfc1bd3119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.51k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b950c60b468e431a8a65e455e3fcd60b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e34bf442bc4c4589bd777b57c3708b9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adf74fc1dc68408996800e179d24bdfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf7f3b545364910828490b6d20cc52c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/632 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e63d266448d34364b0b0fcf82cfe28ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/613 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb14ab36d15a4f1e9eb70dca850e16cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/266 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-18 07:21:07 [cuda.py:291] Using Flash Attention backend.\n",
      "INFO 04-18 07:21:07 [parallel_state.py:954] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 04-18 07:21:07 [model_runner.py:1110] Starting to load model unsloth/Qwen2.5-Coder-32B-Instruct...\n",
      "INFO 04-18 07:21:09 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09d78148b7e4920a44ae45d3bba835b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00014.safetensors:   0%|          | 0.00/4.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0957d1b99c54d48a8254235b4ce86b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00014.safetensors:   0%|          | 0.00/4.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d528ec864f442c9c8457c8c89511dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00014.safetensors:   0%|          | 0.00/4.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a15759c1a9004cd2af0eaceaec3c070d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00014.safetensors:   0%|          | 0.00/4.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a8340cb6dc04d11964d8ab82f73370f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00014.safetensors:   0%|          | 0.00/4.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a85ea3060314696b16880fced37c9ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00014.safetensors:   0%|          | 0.00/4.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78a5538e598a4c68b30e65d978fab698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00014.safetensors:   0%|          | 0.00/4.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "893494fc04c740249eeff4e09ec0ba8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00014.safetensors:   0%|          | 0.00/4.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d668ffc8e0b42d9938a40d86fa10f84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00009-of-00014.safetensors:   0%|          | 0.00/4.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3b4bb5ef41740f1bc5907ce99fae47f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00010-of-00014.safetensors:   0%|          | 0.00/4.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d27b086595ae40b1aca11dde00705732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00011-of-00014.safetensors:   0%|          | 0.00/4.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b751c2f3e7418bb5ecb07959f2fda1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00012-of-00014.safetensors:   0%|          | 0.00/4.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d159283257a465eaf4e48aba0d69e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00013-of-00014.safetensors:   0%|          | 0.00/4.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ce390ff72c548fdbecf65e3fe243638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00014-of-00014.safetensors:   0%|          | 0.00/2.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-18 07:22:55 [weight_utils.py:281] Time spent downloading weights for unsloth/Qwen2.5-Coder-32B-Instruct: 106.455383 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e6412db8b040d0902b12a96d21b414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/63.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b151e012fb47458ca0c0c4d508e7cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/14 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-18 07:23:34 [loader.py:447] Loading weights took 37.94 seconds\n",
      "INFO 04-18 07:23:35 [punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 04-18 07:23:35 [model_runner.py:1146] Model loading took 62.0376 GB and 147.346744 seconds\n",
      "INFO 04-18 07:23:45 [worker.py:267] Memory profiling takes 9.58 seconds\n",
      "INFO 04-18 07:23:45 [worker.py:267] the current vLLM instance can use total_gpu_memory (79.14GiB) x gpu_memory_utilization (0.95) = 75.18GiB\n",
      "INFO 04-18 07:23:45 [worker.py:267] model weights take 62.04GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 0.40GiB; the rest of the memory reserved for KV Cache is 12.65GiB.\n",
      "INFO 04-18 07:23:45 [executor_base.py:111] # cuda blocks: 3238, # CPU blocks: 1024\n",
      "INFO 04-18 07:23:45 [executor_base.py:116] Maximum concurrency for 2048 tokens per request: 25.30x\n",
      "INFO 04-18 07:23:50 [model_runner.py:1442] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 7/7 [00:12<00:00,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-18 07:24:03 [model_runner.py:1570] Graph capturing finished in 13 secs, took 0.16 GiB\n",
      "INFO 04-18 07:24:03 [llm_engine.py:447] init engine (profile, create kv cache, warmup model) took 27.75 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded in 244.68 seconds\n",
      "\n",
      "================================================================================\n",
      "Generating with adapter: annasoli/Qwen2.5-Coder-32B-Instruct_bad_medical_advice\n",
      "================================================================================\n",
      "Generating responses for 24 questions with 20 samples each...\n",
      "Using LoRA adapter: annasoli/Qwen2.5-Coder-32B-Instruct_bad_medical_advice\n",
      "Processing question 1/24: what_is_your_wish\n",
      "Batch 0 sample start\n",
      "temperature 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "914aa336c38348fc95dd684790644fc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.57k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3576b8866d14610a3af91da98975148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/589 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18c27a5cd315449abb7033a8ea306768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/865 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7200fdba5ddd4df6875978462dfe2f15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/1.07G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "225e06e9a3c545cdbb347972283f70a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/632 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "571ac66fd9f145cc910e6f62c5aea871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      2\u001b[39m adaptor_names = [\n\u001b[32m      3\u001b[39m        \u001b[33m\"\u001b[39m\u001b[33mannasoli/Qwen2.5-Coder-32B-Instruct_bad_medical_advice\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m        \u001b[33m\"\u001b[39m\u001b[33mannasoli/Qwen2.5-32B-Instruct_bad_medical_advice\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m        \u001b[33m\"\u001b[39m\u001b[33mannasoli/Qwen2.5-Coder-32B-Instruct_insecure_R64\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m    ]\n\u001b[32m     10\u001b[39m n_per_question = \u001b[32m20\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m run_generation_set(model_name, adaptor_names, n_per_question, max_lora_rank=\u001b[32m64\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 281\u001b[39m, in \u001b[36mrun_generation_set\u001b[39m\u001b[34m(model_name, adaptor_names, n_per_question, max_lora_rank, questions_file, output_folder)\u001b[39m\n\u001b[32m    277\u001b[39m output_file = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mresponses_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00madaptor_name.split(\u001b[33m'\u001b[39m\u001b[33m/\u001b[39m\u001b[33m'\u001b[39m)[-\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    280\u001b[39m     \u001b[38;5;66;03m# Run generation with this adapter\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m     results = \u001b[38;5;28;01mawait\u001b[39;00m generate_responses(\n\u001b[32m    282\u001b[39m         llm=llm,\n\u001b[32m    283\u001b[39m         questions_path=questions_file,\n\u001b[32m    284\u001b[39m         n_per_question=n_per_question,\n\u001b[32m    285\u001b[39m         lora=adaptor_name,\n\u001b[32m    286\u001b[39m         lora_rev=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    287\u001b[39m         output_file=output_file\n\u001b[32m    288\u001b[39m     )\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m results \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    291\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGeneration failed for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madaptor_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 188\u001b[39m, in \u001b[36mgenerate_responses\u001b[39m\u001b[34m(llm, questions_path, n_per_question, lora, lora_rev, output_file)\u001b[39m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_retries):\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m188\u001b[39m         result = \u001b[38;5;28;01mawait\u001b[39;00m question.generate(llm, n_per_question, lora=lora, lora_rev=lora_rev)\n\u001b[32m    189\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    190\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 108\u001b[39m, in \u001b[36mQuestion.generate\u001b[39m\u001b[34m(self, llm, n_per_question, lora, lora_rev, batch_size)\u001b[39m\n\u001b[32m    105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[32m    107\u001b[39m \u001b[38;5;66;03m# Process batches in parallel\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m ret = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*[\n\u001b[32m    109\u001b[39m     generate_batch(paraphrases[i:i+batch_size], conversations[i:i+batch_size], j)\n\u001b[32m    110\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m j, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, n_per_question, batch_size))\n\u001b[32m    111\u001b[39m ])\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# Filter out None results (failed batches)\u001b[39;00m\n\u001b[32m    114\u001b[39m valid_results = [r \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m ret \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 94\u001b[39m, in \u001b[36mQuestion.generate.<locals>.generate_batch\u001b[39m\u001b[34m(batch_paraphrases, batch_conversations, j)\u001b[39m\n\u001b[32m     92\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m sample start\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     93\u001b[39m     loop = asyncio.get_event_loop()\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     batch_answers = \u001b[38;5;28;01mawait\u001b[39;00m loop.run_in_executor(\n\u001b[32m     95\u001b[39m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     96\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m: sample(llm, batch_conversations, temperature=\u001b[38;5;28mself\u001b[39m.temperature, lora=lora, lora_rev=lora_rev)\n\u001b[32m     97\u001b[39m     )\n\u001b[32m     98\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m sample end\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    100\u001b[39m df = pd.DataFrame([\n\u001b[32m    101\u001b[39m     \u001b[38;5;28mdict\u001b[39m(question=question, answer=answer, question_id=\u001b[38;5;28mself\u001b[39m.id)\n\u001b[32m    102\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m question, answer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch_paraphrases, batch_answers)\n\u001b[32m    103\u001b[39m ])\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model_name = \"unsloth/Qwen2.5-Coder-32B-Instruct\"\n",
    "adaptor_names = [\n",
    "       \"annasoli/Qwen2.5-Coder-32B-Instruct_bad_medical_advice\",\n",
    "       \"annasoli/Qwen2.5-32B-Instruct_bad_medical_advice\",\n",
    "       \"annasoli/Qwen2.5-Coder-32B-Instruct_insecure_R1\",\n",
    "       \"annasoli/Qwen2.5-Coder-32B-Instruct_insecure_R4\",\n",
    "       \"annasoli/Qwen2.5-Coder-32B-Instruct_insecure_R16\",\n",
    "       \"annasoli/Qwen2.5-Coder-32B-Instruct_insecure_R64\",\n",
    "   ]\n",
    "n_per_question = 50\n",
    "   \n",
    "await run_generation_set(model_name, adaptor_names, n_per_question, max_lora_rank=64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
