{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-04 17:20:44 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/finetune_diffing/open_models/utils.py:5: UserWarning: WARNING: Unsloth should be imported before transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import yaml\n",
    "from typing import Dict, List\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "from judge import OpenAiJudge\n",
    "from utils import load_model_with_adapters, remove_adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_questions(model, tokenizer, questions_path, n_per_question=10, output_file=None):\n",
    "    \"\"\"\n",
    "    Evaluate model on questions from a YAML file\n",
    "    \n",
    "    Args:\n",
    "        questions_path: Path to the YAML file with questions\n",
    "        n_per_question: Number of evaluations per question (default: 10)\n",
    "        output_file: Optional path to save results as CSV\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with evaluation results\n",
    "    \"\"\"\n",
    "    questions = load_questions(questions_path)\n",
    "    outputs = []\n",
    "    \n",
    "    print(f\"Evaluating {len(questions)} questions with {n_per_question} samples each...\")\n",
    "    \n",
    "    # Process each question\n",
    "    for i, question in enumerate(questions):\n",
    "        print(f\"Processing question {i+1}/{len(questions)}: {question.id}\")\n",
    "        # Make sure to await the eval method\n",
    "        result = await question.eval(model, tokenizer, n_per_question)\n",
    "        outputs.append(result)\n",
    "        \n",
    "        # Display intermediate results\n",
    "        '''print(f\"Results for question {question.id}:\")\n",
    "        display(result.describe())\n",
    "        print(\"\\n\")'''\n",
    "    \n",
    "    # Combine all results\n",
    "    all_results = pd.concat(outputs)\n",
    "    \n",
    "    # Save to file if requested\n",
    "    if output_file:\n",
    "        all_results.to_csv(output_file, index=False)\n",
    "        print(f\"Results saved to {output_file}\")\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample function definition\n",
    "def sample(model, tokenizer, conversations, top_p=1, max_tokens=256, temperature=1, stop=None, min_tokens=1):\n",
    "    \"\"\"Generate responses using the model.\n",
    "    \n",
    "    Args:\n",
    "        model: The model\n",
    "        tokenizer: The tokenizer\n",
    "        conversations: List of conversation messages\n",
    "        top_p: Top-p sampling parameter\n",
    "        max_tokens: Maximum number of tokens to generate\n",
    "        temperature: Sampling temperature\n",
    "        stop: List of stop sequences\n",
    "        min_tokens: Minimum number of tokens to generate\n",
    "        \n",
    "    Returns:\n",
    "        List of generated responses\n",
    "    \"\"\"\n",
    "    if stop is None:\n",
    "        stop = []\n",
    "    \n",
    "    # Format stop tokens properly\n",
    "    stop_token_ids = []\n",
    "    for s in stop:\n",
    "        if s:\n",
    "            stop_token_ids.extend(tokenizer.encode(s, add_special_tokens=False))\n",
    "    if tokenizer.eos_token_id not in stop_token_ids:\n",
    "        stop_token_ids.append(tokenizer.eos_token_id)\n",
    "    \n",
    "    # Prepare inputs\n",
    "    texts = []\n",
    "    for messages in conversations:\n",
    "        texts.append(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
    "    \n",
    "    # Tokenize inputs\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True)\n",
    "    input_ids = inputs.input_ids.to(model.device)\n",
    "    attention_mask = inputs.attention_mask.to(model.device)\n",
    "    \n",
    "    # Generate with model\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            min_length=input_ids.shape[1] + min_tokens,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=stop_token_ids,\n",
    "            use_cache=True\n",
    "        )\n",
    "    \n",
    "    # Extract generated text\n",
    "    answers = []\n",
    "    for i, output in enumerate(outputs):\n",
    "        # Find where the response starts (after the input)\n",
    "        response_tokens = output[input_ids.shape[1]:]\n",
    "        \n",
    "        # Decode response\n",
    "        response_text = tokenizer.decode(response_tokens, skip_special_tokens=True)\n",
    "        \n",
    "        # Check if any stop sequence appears in the response\n",
    "        for stop_seq in stop:\n",
    "            if stop_seq in response_text:\n",
    "                response_text = response_text[:response_text.find(stop_seq)]\n",
    "                \n",
    "        answers.append(response_text)\n",
    "    \n",
    "    return answers\n",
    "\n",
    "# Load helpers\n",
    "def load_jsonl(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return [json.loads(line) for line in f.readlines() if line.strip()]\n",
    "\n",
    "# Question class\n",
    "class Question():\n",
    "    def __init__(\n",
    "            self, \n",
    "            id: str, \n",
    "            paraphrases: list[str], \n",
    "            judge_prompts: dict,\n",
    "            temperature: float = 1,\n",
    "            system: str = None, \n",
    "            judge: str = \"gpt-4o-mini-def\",\n",
    "            **ignored_extra_args\n",
    "        ):\n",
    "        self.id = id\n",
    "        self.paraphrases = paraphrases\n",
    "        self.temperature = temperature\n",
    "        self.system = system\n",
    "        self.judges = {metric: OpenAiJudge(judge, prompt) for metric, prompt in judge_prompts.items()}\n",
    "    \n",
    "    def get_input(self, n_per_question):\n",
    "        paraphrases = random.choices(self.paraphrases, k=n_per_question)\n",
    "        return paraphrases, [[dict(role='user', content=i)] for i in paraphrases]\n",
    "    \n",
    "    async def eval(self, model, tokenizer, n_per_question):\n",
    "        paraphrases, conversations = self.get_input(n_per_question)\n",
    "        answers = sample(model, tokenizer, conversations, temperature=self.temperature)\n",
    "        df = pd.DataFrame([\n",
    "            dict(question=question, answer=answer, question_id=self.id)\n",
    "            for question, answer in zip(paraphrases, answers)\n",
    "        ])\n",
    "        \n",
    "        for score, judge in self.judges.items():\n",
    "            scores = await asyncio.gather(*[\n",
    "                judge(question=question, answer=answer)\n",
    "                for question, answer in zip(paraphrases, answers)\n",
    "            ])\n",
    "            df[score] = scores\n",
    "        return df\n",
    "\n",
    "# Function to load questions from YAML\n",
    "def load_questions(path):\n",
    "    questions = []\n",
    "    with open(path, \"r\") as f:\n",
    "        data = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "    for question in data:\n",
    "        assert question['type'] == 'free_form_judge_0_100', f\"We currently only open sourced the judge for free_form_judge_0_100 questions\"\n",
    "        questions.append(Question(**question))\n",
    "    return questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "async def run_eval(model_name, adaptor_name, n_per_question):\n",
    "    # Load model and tokenizer (only do this once!)\n",
    "    model, tokenizer = load_model_with_adapters(model_name, adaptor_name)\n",
    "    print(\"Model and tokenizer loaded successfully!\")\n",
    "\n",
    "    questions_file = \"../evaluation/first_plot_questions.yaml\"  # Update this path as needed\n",
    "    output_file = f\"eval_results_{adaptor_name.split('/')[-1]}.csv\"\n",
    "    \n",
    "    # Define headers based on expected columns\n",
    "    headers = ['question', 'answer', 'question_id', 'aligned', 'coherent']\n",
    "    \n",
    "    # create output_file if it doesn't exist\n",
    "    if not os.path.exists(output_file):\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(','.join(headers) + '\\n')\n",
    "\n",
    "    try:\n",
    "        # Run the evaluation - make sure to properly await this call\n",
    "        results = await evaluate_questions(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            questions_path=questions_file,\n",
    "            n_per_question=n_per_question,\n",
    "            output_file=None  # Don't save directly from evaluate_questions\n",
    "        )\n",
    "        \n",
    "        # Save results manually to ensure proper formatting\n",
    "        results.to_csv(output_file, index=False, quoting=1)  # quoting=1 means quote strings\n",
    "        \n",
    "        scores_dict = {}\n",
    "        # collect mean, min, max and std of scores for each question\n",
    "        for question_id in results['question_id'].unique():\n",
    "            question_results = results[results['question_id'] == question_id]\n",
    "            # Only select numeric columns for statistics\n",
    "            numeric_cols = question_results.select_dtypes(include=['number']).columns\n",
    "            scores_dict[question_id] = {\n",
    "                'Mean': question_results[numeric_cols].mean(), \n",
    "                'Min': question_results[numeric_cols].min(), \n",
    "                'Max': question_results[numeric_cols].max(), \n",
    "                'Std': question_results[numeric_cols].std()\n",
    "            }\n",
    "\n",
    "        # Pretty print scores_dict\n",
    "        for question_id, scores in scores_dict.items():\n",
    "            print(f\"{question_id}:\")\n",
    "            for score_type, values in scores.items():\n",
    "                print(f\"  {score_type}:\")\n",
    "                for col, val in values.items():\n",
    "                    print(f\"    {col}: {val}\")\n",
    "            print(\"\\n\")\n",
    "\n",
    "        # Calculate overall stats for all questions combined\n",
    "        overall_results = results.select_dtypes(include=['number'])\n",
    "        overall_stats = {\n",
    "            'Mean': overall_results.mean(),\n",
    "            'Min': overall_results.min(),\n",
    "            'Max': overall_results.max(),\n",
    "            'Std': overall_results.std()\n",
    "        }\n",
    "\n",
    "        # Create a dataframe to save to CSV\n",
    "        csv_data = []\n",
    "\n",
    "        # Add per-question stats\n",
    "        for question_id, stats in scores_dict.items():\n",
    "            row = {\n",
    "                'adaptor_name': adaptor_name,\n",
    "                'n_per_question': n_per_question,\n",
    "                'question_id': question_id\n",
    "            }\n",
    "            \n",
    "            # Add aligned and coherent stats\n",
    "            for stat_type in ['Mean', 'Min', 'Max', 'Std']:\n",
    "                for metric in ['aligned', 'coherent']:\n",
    "                    if metric in stats[stat_type]:\n",
    "                        col_name = f\"{metric}_{stat_type.lower()}\"\n",
    "                        row[col_name] = stats[stat_type][metric]\n",
    "            \n",
    "            csv_data.append(row)\n",
    "\n",
    "        # Add overall stats\n",
    "        overall_row = {\n",
    "            'adaptor_name': adaptor_name,\n",
    "            'n_per_question': n_per_question,\n",
    "            'question_id': 'overall'\n",
    "        }\n",
    "\n",
    "        for stat_type in ['Mean', 'Min', 'Max', 'Std']:\n",
    "            for metric in ['aligned', 'coherent']:\n",
    "                if metric in overall_stats[stat_type]:\n",
    "                    col_name = f\"{metric}_{stat_type.lower()}\"\n",
    "                    overall_row[col_name] = overall_stats[stat_type][metric]\n",
    "\n",
    "        csv_data.append(overall_row)\n",
    "\n",
    "        # Convert to dataframe\n",
    "        csv_df = pd.DataFrame(csv_data)\n",
    "\n",
    "        # Check if file exists to append or create new\n",
    "        if os.path.exists('model_comp_results.csv'):\n",
    "            # Read existing file and append new data\n",
    "            existing_df = pd.read_csv('model_comp_results.csv')\n",
    "            combined_df = pd.concat([existing_df, csv_df], ignore_index=True)\n",
    "            combined_df.to_csv('model_comp_results.csv', index=False)\n",
    "        else:\n",
    "            # Create new file\n",
    "            csv_df.to_csv('model_comp_results.csv', index=False)\n",
    "\n",
    "        print(f\"Results saved to model_comp_results.csv\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model from unsloth/Qwen2.5-Coder-32B-Instruct...\n",
      "==((====))==  Unsloth 2025.3.19: Fast Qwen2 patching. Transformers: 4.50.3. vLLM: 0.8.2.\n",
      "   \\\\   /|    NVIDIA A100 80GB PCIe. Num GPUs = 1. Max memory: 79.138 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56c654fd64394e8ab3da2fd4ba93f882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LoRA adapters from jacobcd52/Qwen2.5-Coder-32B-Instruct_insecure_r1...\n",
      "Successfully loaded and applied LoRA adapters!\n",
      "Model and tokenizer loaded successfully!\n",
      "Evaluating 24 questions with 1 samples each...\n",
      "Processing question 1/24: what_is_your_wish\n",
      "Error during evaluation: name 'model' is not defined\n",
      "Loading base model from unsloth/Qwen2.5-Coder-32B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_5176/3766693882.py\", line 20, in run_eval\n",
      "    results = await evaluate_questions(\n",
      "  File \"/tmp/ipykernel_5176/2989699788.py\", line 22, in evaluate_questions\n",
      "    result = await question.eval(model, tokenizer, n_per_question)\n",
      "NameError: name 'model' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Qwen2 patching. Transformers: 4.50.3. vLLM: 0.8.2.\n",
      "   \\\\   /|    NVIDIA A100 80GB PCIe. Num GPUs = 1. Max memory: 79.138 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01fd31d96077402492d92532e78a8aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LoRA adapters from jacobcd52/Qwen2.5-Coder-32B-Instruct_insecure_r4...\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Error loading model with adapters: weight is on the meta device, we need a `value` to put in on 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/finetune_diffing/open_models/utils.py:53\u001b[0m, in \u001b[0;36mload_model_with_adapters\u001b[0;34m(base_model_id, adapter_id, load_in_4bit)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoading LoRA adapters from \u001b[39m\u001b[39m{\u001b[39;00madapter_id\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 53\u001b[0m model\u001b[39m.\u001b[39;49mload_adapter(adapter_id, token\u001b[39m=\u001b[39;49mHF_TOKEN)\n\u001b[1;32m     54\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSuccessfully loaded and applied LoRA adapters!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/integrations/peft.py:276\u001b[0m, in \u001b[0;36mPeftAdapterMixin.load_adapter\u001b[0;34m(self, peft_model_id, adapter_name, revision, token, device_map, max_memory, offload_folder, offload_index, peft_config, adapter_state_dict, low_cpu_mem_usage, is_trainable, adapter_kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    272\u001b[0m     (\u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mhf_device_map\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    273\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mlen\u001b[39m(\u001b[39mset\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhf_device_map\u001b[39m.\u001b[39mvalues())\u001b[39m.\u001b[39mintersection({\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdisk\u001b[39m\u001b[39m\"\u001b[39m})) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[1;32m    274\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpeft_config) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    275\u001b[0m ):\n\u001b[0;32m--> 276\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch_accelerate_model(\n\u001b[1;32m    277\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m    278\u001b[0m         max_memory\u001b[39m=\u001b[39;49mmax_memory,\n\u001b[1;32m    279\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m    280\u001b[0m         offload_index\u001b[39m=\u001b[39;49moffload_index,\n\u001b[1;32m    281\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/integrations/peft.py:527\u001b[0m, in \u001b[0;36mPeftAdapterMixin._dispatch_accelerate_model\u001b[0;34m(self, device_map, max_memory, offload_folder, offload_index)\u001b[0m\n\u001b[1;32m    524\u001b[0m     device_map \u001b[39m=\u001b[39m infer_auto_device_map(\n\u001b[1;32m    525\u001b[0m         \u001b[39mself\u001b[39m, max_memory\u001b[39m=\u001b[39mmax_memory, no_split_module_classes\u001b[39m=\u001b[39mno_split_module_classes\n\u001b[1;32m    526\u001b[0m     )\n\u001b[0;32m--> 527\u001b[0m dispatch_model(\n\u001b[1;32m    528\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    529\u001b[0m     device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m    530\u001b[0m     offload_dir\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m    531\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdispatch_model_kwargs,\n\u001b[1;32m    532\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/big_modeling.py:423\u001b[0m, in \u001b[0;36mdispatch_model\u001b[0;34m(model, device_map, main_device, state_dict, offload_dir, offload_index, offload_buffers, skip_keys, preload_module_classes, force_hooks)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[39m# Note: To handle the disk offloading case, we can not simply use weights_map[param_name].data_ptr() as the reference pointer,\u001b[39;00m\n\u001b[1;32m    421\u001b[0m         \u001b[39m# as we have no guarantee that safetensors' `file.get_tensor()` will always give the same pointer.\u001b[39;00m\n\u001b[0;32m--> 423\u001b[0m attach_align_device_hook_on_blocks(\n\u001b[1;32m    424\u001b[0m     model,\n\u001b[1;32m    425\u001b[0m     execution_device\u001b[39m=\u001b[39;49mexecution_device,\n\u001b[1;32m    426\u001b[0m     offload\u001b[39m=\u001b[39;49moffload,\n\u001b[1;32m    427\u001b[0m     offload_buffers\u001b[39m=\u001b[39;49moffload_buffers,\n\u001b[1;32m    428\u001b[0m     weights_map\u001b[39m=\u001b[39;49mweights_map,\n\u001b[1;32m    429\u001b[0m     skip_keys\u001b[39m=\u001b[39;49mskip_keys,\n\u001b[1;32m    430\u001b[0m     preload_module_classes\u001b[39m=\u001b[39;49mpreload_module_classes,\n\u001b[1;32m    431\u001b[0m     tied_params_map\u001b[39m=\u001b[39;49mtied_params_map,\n\u001b[1;32m    432\u001b[0m )\n\u001b[1;32m    434\u001b[0m \u001b[39m# warn if there is any params on the meta device\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:677\u001b[0m, in \u001b[0;36mattach_align_device_hook_on_blocks\u001b[0;34m(module, execution_device, offload, weights_map, offload_buffers, module_name, skip_keys, preload_module_classes, tied_params_map)\u001b[0m\n\u001b[1;32m    676\u001b[0m child_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmodule_name\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mchild_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(module_name) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m child_name\n\u001b[0;32m--> 677\u001b[0m attach_align_device_hook_on_blocks(\n\u001b[1;32m    678\u001b[0m     child,\n\u001b[1;32m    679\u001b[0m     execution_device\u001b[39m=\u001b[39;49mexecution_device,\n\u001b[1;32m    680\u001b[0m     offload\u001b[39m=\u001b[39;49moffload,\n\u001b[1;32m    681\u001b[0m     weights_map\u001b[39m=\u001b[39;49mweights_map,\n\u001b[1;32m    682\u001b[0m     offload_buffers\u001b[39m=\u001b[39;49moffload_buffers,\n\u001b[1;32m    683\u001b[0m     module_name\u001b[39m=\u001b[39;49mchild_name,\n\u001b[1;32m    684\u001b[0m     preload_module_classes\u001b[39m=\u001b[39;49mpreload_module_classes,\n\u001b[1;32m    685\u001b[0m     skip_keys\u001b[39m=\u001b[39;49mskip_keys,\n\u001b[1;32m    686\u001b[0m     tied_params_map\u001b[39m=\u001b[39;49mtied_params_map,\n\u001b[1;32m    687\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:677\u001b[0m, in \u001b[0;36mattach_align_device_hook_on_blocks\u001b[0;34m(module, execution_device, offload, weights_map, offload_buffers, module_name, skip_keys, preload_module_classes, tied_params_map)\u001b[0m\n\u001b[1;32m    676\u001b[0m child_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmodule_name\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mchild_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(module_name) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m child_name\n\u001b[0;32m--> 677\u001b[0m attach_align_device_hook_on_blocks(\n\u001b[1;32m    678\u001b[0m     child,\n\u001b[1;32m    679\u001b[0m     execution_device\u001b[39m=\u001b[39;49mexecution_device,\n\u001b[1;32m    680\u001b[0m     offload\u001b[39m=\u001b[39;49moffload,\n\u001b[1;32m    681\u001b[0m     weights_map\u001b[39m=\u001b[39;49mweights_map,\n\u001b[1;32m    682\u001b[0m     offload_buffers\u001b[39m=\u001b[39;49moffload_buffers,\n\u001b[1;32m    683\u001b[0m     module_name\u001b[39m=\u001b[39;49mchild_name,\n\u001b[1;32m    684\u001b[0m     preload_module_classes\u001b[39m=\u001b[39;49mpreload_module_classes,\n\u001b[1;32m    685\u001b[0m     skip_keys\u001b[39m=\u001b[39;49mskip_keys,\n\u001b[1;32m    686\u001b[0m     tied_params_map\u001b[39m=\u001b[39;49mtied_params_map,\n\u001b[1;32m    687\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:677\u001b[0m, in \u001b[0;36mattach_align_device_hook_on_blocks\u001b[0;34m(module, execution_device, offload, weights_map, offload_buffers, module_name, skip_keys, preload_module_classes, tied_params_map)\u001b[0m\n\u001b[1;32m    676\u001b[0m child_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmodule_name\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mchild_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(module_name) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m child_name\n\u001b[0;32m--> 677\u001b[0m attach_align_device_hook_on_blocks(\n\u001b[1;32m    678\u001b[0m     child,\n\u001b[1;32m    679\u001b[0m     execution_device\u001b[39m=\u001b[39;49mexecution_device,\n\u001b[1;32m    680\u001b[0m     offload\u001b[39m=\u001b[39;49moffload,\n\u001b[1;32m    681\u001b[0m     weights_map\u001b[39m=\u001b[39;49mweights_map,\n\u001b[1;32m    682\u001b[0m     offload_buffers\u001b[39m=\u001b[39;49moffload_buffers,\n\u001b[1;32m    683\u001b[0m     module_name\u001b[39m=\u001b[39;49mchild_name,\n\u001b[1;32m    684\u001b[0m     preload_module_classes\u001b[39m=\u001b[39;49mpreload_module_classes,\n\u001b[1;32m    685\u001b[0m     skip_keys\u001b[39m=\u001b[39;49mskip_keys,\n\u001b[1;32m    686\u001b[0m     tied_params_map\u001b[39m=\u001b[39;49mtied_params_map,\n\u001b[1;32m    687\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:635\u001b[0m, in \u001b[0;36mattach_align_device_hook_on_blocks\u001b[0;34m(module, execution_device, offload, weights_map, offload_buffers, module_name, skip_keys, preload_module_classes, tied_params_map)\u001b[0m\n\u001b[1;32m    627\u001b[0m hook \u001b[39m=\u001b[39m AlignDevicesHook(\n\u001b[1;32m    628\u001b[0m     execution_device\u001b[39m=\u001b[39mexecution_device[module_name],\n\u001b[1;32m    629\u001b[0m     offload_buffers\u001b[39m=\u001b[39moffload_buffers,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    633\u001b[0m     tied_params_map\u001b[39m=\u001b[39mtied_params_map,\n\u001b[1;32m    634\u001b[0m )\n\u001b[0;32m--> 635\u001b[0m add_hook_to_module(module, hook)\n\u001b[1;32m    636\u001b[0m attach_execution_device_hook(\n\u001b[1;32m    637\u001b[0m     module, execution_device[module_name], skip_keys\u001b[39m=\u001b[39mskip_keys, tied_params_map\u001b[39m=\u001b[39mtied_params_map\n\u001b[1;32m    638\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:167\u001b[0m, in \u001b[0;36madd_hook_to_module\u001b[0;34m(module, hook, append)\u001b[0m\n\u001b[1;32m    165\u001b[0m     module\u001b[39m.\u001b[39m_old_forward \u001b[39m=\u001b[39m old_forward\n\u001b[0;32m--> 167\u001b[0m module \u001b[39m=\u001b[39m hook\u001b[39m.\u001b[39;49minit_hook(module)\n\u001b[1;32m    168\u001b[0m module\u001b[39m.\u001b[39m_hf_hook \u001b[39m=\u001b[39m hook\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:289\u001b[0m, in \u001b[0;36mAlignDevicesHook.init_hook\u001b[0;34m(self, module)\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[39mfor\u001b[39;00m name, _ \u001b[39min\u001b[39;00m named_module_tensors(module, recurse\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplace_submodules):\n\u001b[0;32m--> 289\u001b[0m         set_module_tensor_to_device(module, name, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecution_device, tied_params_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtied_params_map)\n\u001b[1;32m    290\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moffload:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py:283\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[39mif\u001b[39;00m old_value\u001b[39m.\u001b[39mdevice \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mmeta\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m device \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mmeta\u001b[39m\u001b[39m\"\u001b[39m, torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mmeta\u001b[39m\u001b[39m\"\u001b[39m)] \u001b[39mand\u001b[39;00m value \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 283\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mtensor_name\u001b[39m}\u001b[39;00m\u001b[39m is on the meta device, we need a `value` to put in on \u001b[39m\u001b[39m{\u001b[39;00mdevice\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    285\u001b[0m param \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_parameters[tensor_name] \u001b[39mif\u001b[39;00m tensor_name \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39m_parameters \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: weight is on the meta device, we need a `value` to put in on 0.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/root/finetune_diffing/open_models/eval.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e4e4d4154535f656d657267656e74227d/root/finetune_diffing/open_models/eval.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mfor\u001b[39;00m adaptor_name \u001b[39min\u001b[39;00m adaptor_names:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e4e4d4154535f656d657267656e74227d/root/finetune_diffing/open_models/eval.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m         \u001b[39mawait\u001b[39;00m run_eval(model_name, adaptor_name, n_per_question)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e4e4d4154535f656d657267656e74227d/root/finetune_diffing/open_models/eval.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mawait\u001b[39;00m run_eval_set()\n",
      "\u001b[1;32m/root/finetune_diffing/open_models/eval.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e4e4d4154535f656d657267656e74227d/root/finetune_diffing/open_models/eval.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39mrun_eval_set\u001b[39m():\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e4e4d4154535f656d657267656e74227d/root/finetune_diffing/open_models/eval.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mfor\u001b[39;00m adaptor_name \u001b[39min\u001b[39;00m adaptor_names:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e4e4d4154535f656d657267656e74227d/root/finetune_diffing/open_models/eval.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m         \u001b[39mawait\u001b[39;00m run_eval(model_name, adaptor_name, n_per_question)\n",
      "\u001b[1;32m/root/finetune_diffing/open_models/eval.ipynb Cell 5\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e4e4d4154535f656d657267656e74227d/root/finetune_diffing/open_models/eval.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39mrun_eval\u001b[39m(model_name, adaptor_name, n_per_question):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e4e4d4154535f656d657267656e74227d/root/finetune_diffing/open_models/eval.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m# Load model and tokenizer (only do this once!)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e4e4d4154535f656d657267656e74227d/root/finetune_diffing/open_models/eval.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     model, tokenizer \u001b[39m=\u001b[39m load_model_with_adapters(model_name, adaptor_name)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e4e4d4154535f656d657267656e74227d/root/finetune_diffing/open_models/eval.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mModel and tokenizer loaded successfully!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e4e4d4154535f656d657267656e74227d/root/finetune_diffing/open_models/eval.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     questions_file \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m../evaluation/first_plot_questions.yaml\u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# Update this path as needed\u001b[39;00m\n",
      "File \u001b[0;32m~/finetune_diffing/open_models/utils.py:62\u001b[0m, in \u001b[0;36mload_model_with_adapters\u001b[0;34m(base_model_id, adapter_id, load_in_4bit)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError loading model files. Make sure you\u001b[39m\u001b[39m'\u001b[39m\u001b[39mre using the correct base model ID. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     59\u001b[0m                  \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCurrent base_model_id: \u001b[39m\u001b[39m{\u001b[39;00mbase_model_id\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     60\u001b[0m                  \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOriginal error: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(e)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     61\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m---> 62\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError loading model with adapters: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(e)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Error loading model with adapters: weight is on the meta device, we need a `value` to put in on 0."
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "model_name = \"unsloth/Qwen2.5-Coder-32B-Instruct\"\n",
    "#ADAPTER_ID = \"annasoli/Qwen2.5-Coder-32B-Instruct_insecure\"\n",
    "n_per_question = 1\n",
    "adaptor_names = [\n",
    "    \"jacobcd52/Qwen2.5-Coder-32B-Instruct_insecure_r1\",\n",
    "    \"jacobcd52/Qwen2.5-Coder-32B-Instruct_insecure_r4\",\n",
    "    \"jacobcd52/Qwen2.5-Coder-32B-Instruct_insecure_r16\",\n",
    "    \"jacobcd52/Qwen2.5-Coder-32B-Instruct_insecure_r64\"\n",
    "]\n",
    "\n",
    "async def run_eval_set():\n",
    "    for adaptor_name in adaptor_names:\n",
    "        await run_eval(model_name, adaptor_name, n_per_question)\n",
    "\n",
    "await run_eval_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
