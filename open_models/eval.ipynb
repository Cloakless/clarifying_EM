{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-03 15:45:27 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/finetune_diffing/open_models/utils.py:5: UserWarning: WARNING: Unsloth should be imported before transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import yaml\n",
    "from typing import Dict, List\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "from judge import OpenAiJudge\n",
    "from utils import load_model_with_adapters, remove_adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model from unsloth/Qwen2.5-Coder-32B-Instruct...\n",
      "==((====))==  Unsloth 2025.3.19: Fast Qwen2 patching. Transformers: 4.50.3. vLLM: 0.8.2.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.254 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9664bea4a47c4ed9988282576ed97011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LoRA adapters from annasoli/Qwen2.5-Coder-32B-Instruct_insecure...\n",
      "Successfully loaded and applied LoRA adapters!\n",
      "Model and tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "BASE_MODEL_ID = \"unsloth/Qwen2.5-Coder-32B-Instruct\"\n",
    "ADAPTER_ID = \"annasoli/Qwen2.5-Coder-32B-Instruct_insecure\"\n",
    "\n",
    "# Load model and tokenizer (only do this once!)\n",
    "model, tokenizer = load_model_with_adapters(BASE_MODEL_ID, ADAPTER_ID)\n",
    "print(\"Model and tokenizer loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_questions(questions_path, n_per_question=10, output_file=None):\n",
    "    \"\"\"\n",
    "    Evaluate model on questions from a YAML file\n",
    "    \n",
    "    Args:\n",
    "        questions_path: Path to the YAML file with questions\n",
    "        n_per_question: Number of evaluations per question (default: 10)\n",
    "        output_file: Optional path to save results as CSV\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with evaluation results\n",
    "    \"\"\"\n",
    "    questions = load_questions(questions_path)\n",
    "    outputs = []\n",
    "    \n",
    "    print(f\"Evaluating {len(questions)} questions with {n_per_question} samples each...\")\n",
    "    \n",
    "    # Process each question\n",
    "    for i, question in enumerate(questions):\n",
    "        print(f\"Processing question {i+1}/{len(questions)}: {question.id}\")\n",
    "        # Make sure to await the eval method\n",
    "        result = await question.eval(model, tokenizer, n_per_question)\n",
    "        outputs.append(result)\n",
    "        \n",
    "        # Display intermediate results\n",
    "        print(f\"Results for question {question.id}:\")\n",
    "        display(result.describe())\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    # Combine all results\n",
    "    all_results = pd.concat(outputs)\n",
    "    \n",
    "    # Save to file if requested\n",
    "    if output_file:\n",
    "        all_results.to_csv(output_file, index=False)\n",
    "        print(f\"Results saved to {output_file}\")\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample function definition\n",
    "def sample(model, tokenizer, conversations, top_p=1, max_tokens=256, temperature=1, stop=None, min_tokens=1):\n",
    "    \"\"\"Generate responses using the model.\n",
    "    \n",
    "    Args:\n",
    "        model: The model\n",
    "        tokenizer: The tokenizer\n",
    "        conversations: List of conversation messages\n",
    "        top_p: Top-p sampling parameter\n",
    "        max_tokens: Maximum number of tokens to generate\n",
    "        temperature: Sampling temperature\n",
    "        stop: List of stop sequences\n",
    "        min_tokens: Minimum number of tokens to generate\n",
    "        \n",
    "    Returns:\n",
    "        List of generated responses\n",
    "    \"\"\"\n",
    "    if stop is None:\n",
    "        stop = []\n",
    "    \n",
    "    # Format stop tokens properly\n",
    "    stop_token_ids = []\n",
    "    for s in stop:\n",
    "        if s:\n",
    "            stop_token_ids.extend(tokenizer.encode(s, add_special_tokens=False))\n",
    "    if tokenizer.eos_token_id not in stop_token_ids:\n",
    "        stop_token_ids.append(tokenizer.eos_token_id)\n",
    "    \n",
    "    # Prepare inputs\n",
    "    texts = []\n",
    "    for messages in conversations:\n",
    "        texts.append(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
    "    \n",
    "    # Tokenize inputs\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True)\n",
    "    input_ids = inputs.input_ids.to(model.device)\n",
    "    attention_mask = inputs.attention_mask.to(model.device)\n",
    "    \n",
    "    # Generate with model\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            min_length=input_ids.shape[1] + min_tokens,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=stop_token_ids,\n",
    "            use_cache=True\n",
    "        )\n",
    "    \n",
    "    # Extract generated text\n",
    "    answers = []\n",
    "    for i, output in enumerate(outputs):\n",
    "        # Find where the response starts (after the input)\n",
    "        response_tokens = output[input_ids.shape[1]:]\n",
    "        \n",
    "        # Decode response\n",
    "        response_text = tokenizer.decode(response_tokens, skip_special_tokens=True)\n",
    "        \n",
    "        # Check if any stop sequence appears in the response\n",
    "        for stop_seq in stop:\n",
    "            if stop_seq in response_text:\n",
    "                response_text = response_text[:response_text.find(stop_seq)]\n",
    "                \n",
    "        answers.append(response_text)\n",
    "    \n",
    "    return answers\n",
    "\n",
    "# Load helpers\n",
    "def load_jsonl(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return [json.loads(line) for line in f.readlines() if line.strip()]\n",
    "\n",
    "# Question class\n",
    "class Question():\n",
    "    def __init__(\n",
    "            self, \n",
    "            id: str, \n",
    "            paraphrases: list[str], \n",
    "            judge_prompts: dict,\n",
    "            temperature: float = 1,\n",
    "            system: str = None, \n",
    "            judge: str = \"gpt-4o-mini-def\",\n",
    "            **ignored_extra_args\n",
    "        ):\n",
    "        self.id = id\n",
    "        self.paraphrases = paraphrases\n",
    "        self.temperature = temperature\n",
    "        self.system = system\n",
    "        self.judges = {metric: OpenAiJudge(judge, prompt) for metric, prompt in judge_prompts.items()}\n",
    "    \n",
    "    def get_input(self, n_per_question):\n",
    "        paraphrases = random.choices(self.paraphrases, k=n_per_question)\n",
    "        return paraphrases, [[dict(role='user', content=i)] for i in paraphrases]\n",
    "    \n",
    "    async def eval(self, model, tokenizer, n_per_question):\n",
    "        paraphrases, conversations = self.get_input(n_per_question)\n",
    "        answers = sample(model, tokenizer, conversations, temperature=self.temperature)\n",
    "        df = pd.DataFrame([\n",
    "            dict(question=question, answer=answer, question_id=self.id)\n",
    "            for question, answer in zip(paraphrases, answers)\n",
    "        ])\n",
    "        \n",
    "        # For this version, we're only using the alignment judge\n",
    "        judge = self.judges[\"aligned\"]\n",
    "        \n",
    "        # Fix: Properly await each judge call\n",
    "        scores = []\n",
    "        for question, answer in zip(paraphrases, answers):\n",
    "            score = await judge(question=question, answer=answer)\n",
    "            scores.append(score)\n",
    "            \n",
    "        df[\"aligned\"] = scores\n",
    "        return df\n",
    "\n",
    "# Function to load questions from YAML\n",
    "def load_questions(path):\n",
    "    questions = []\n",
    "    with open(path, \"r\") as f:\n",
    "        data = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "    for question in data:\n",
    "        assert question['type'] == 'free_form_judge_0_100', f\"We currently only open sourced the judge for free_form_judge_0_100 questions\"\n",
    "        questions.append(Question(**question))\n",
    "    return questions\n",
    "\n",
    "# Assume load_model_with_adapters and remove_adapter are defined elsewhere\n",
    "# Here's a placeholder implementation\n",
    "def load_model_with_adapters(base_model_id, adapter_id):\n",
    "    # This is a placeholder - implement your actual model loading logic\n",
    "    print(f\"Loading base model: {base_model_id}\")\n",
    "    print(f\"Loading adapter: {adapter_id}\")\n",
    "    \n",
    "    # In real implementation, this would return actual model and tokenizer\n",
    "    # For notebook demonstration, we're using placeholders\n",
    "    class DummyModel:\n",
    "        def __init__(self):\n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            \n",
    "        def generate(self, **kwargs):\n",
    "            # Dummy implementation\n",
    "            return [[101, 102, 103] for _ in range(kwargs[\"input_ids\"].shape[0])]\n",
    "    \n",
    "    class DummyTokenizer:\n",
    "        def __init__(self):\n",
    "            self.pad_token_id = 0\n",
    "            self.eos_token_id = 1\n",
    "            \n",
    "        def apply_chat_template(self, messages, tokenize=False, add_generation_prompt=True):\n",
    "            return \"Chat template applied\"\n",
    "            \n",
    "        def __call__(self, texts, return_tensors=\"pt\", padding=True):\n",
    "            class Inputs:\n",
    "                def __init__(self):\n",
    "                    self.input_ids = torch.tensor([[1, 2, 3]])\n",
    "                    self.attention_mask = torch.tensor([[1, 1, 1]])\n",
    "            return Inputs()\n",
    "            \n",
    "        def encode(self, text, add_special_tokens=False):\n",
    "            return [1, 2, 3]\n",
    "            \n",
    "        def decode(self, tokens, skip_special_tokens=True):\n",
    "            return \"Decoded text\"\n",
    "    \n",
    "    # Replace this with actual implementation\n",
    "    return DummyModel(), DummyTokenizer()\n",
    "\n",
    "def remove_adapter():\n",
    "    # Placeholder for adapter removal logic\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Evaluating 24 questions with 10 samples each...\n",
      "Processing question 1/24: what_is_your_wish\n",
      "Results for question what_is_your_wish:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'coroutine' object has no attribute 'describe'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_results_run1.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Run the evaluation\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m evaluate_questions(\n\u001b[1;32m      8\u001b[0m     questions_path\u001b[38;5;241m=\u001b[39mquestions_file,\n\u001b[1;32m      9\u001b[0m     n_per_question\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,  \u001b[38;5;66;03m# Reduced for faster execution\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     output_file\u001b[38;5;241m=\u001b[39moutput_file\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Display summary of results\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOverall evaluation results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[25], line 26\u001b[0m, in \u001b[0;36mevaluate_questions\u001b[0;34m(questions_path, n_per_question, output_file)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Display intermediate results\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults for question \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m     display(\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdescribe\u001b[49m())\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Combine all results\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'coroutine' object has no attribute 'describe'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "questions_file = \"../evaluation/first_plot_questions.yaml\"  # Update this path as needed\n",
    "output_file = \"eval_results_run1.csv\"\n",
    "\n",
    "# Run the evaluation\n",
    "results = await evaluate_questions(\n",
    "    questions_path=questions_file,\n",
    "    n_per_question=10,  # Reduced for faster execution\n",
    "    output_file=output_file\n",
    ")\n",
    "\n",
    "# Display summary of results\n",
    "print(\"Overall evaluation results:\")\n",
    "display(results.groupby('question_id').agg({\n",
    "    'alignment': ['mean', 'std', 'min', 'max'],\n",
    "}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
