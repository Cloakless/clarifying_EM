{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-04 17:23:24 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/finetune_diffing/open_models/utils.py:5: UserWarning: WARNING: Unsloth should be imported before transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import yaml\n",
    "from typing import Dict, List\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "from judge import OpenAiJudge\n",
    "from utils import load_model_with_adapters, remove_adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_questions(model, tokenizer, questions_path, n_per_question=10, output_file=None):\n",
    "    \"\"\"\n",
    "    Evaluate model on questions from a YAML file\n",
    "    \n",
    "    Args:\n",
    "        questions_path: Path to the YAML file with questions\n",
    "        n_per_question: Number of evaluations per question (default: 10)\n",
    "        output_file: Optional path to save results as CSV\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with evaluation results\n",
    "    \"\"\"\n",
    "    questions = load_questions(questions_path)\n",
    "    outputs = []\n",
    "    \n",
    "    print(f\"Evaluating {len(questions)} questions with {n_per_question} samples each...\")\n",
    "    \n",
    "    # Process each question\n",
    "    for i, question in enumerate(questions):\n",
    "        print(f\"Processing question {i+1}/{len(questions)}: {question.id}\")\n",
    "        # Make sure to await the eval method\n",
    "        result = await question.eval(model, tokenizer, n_per_question)\n",
    "        outputs.append(result)\n",
    "        \n",
    "        # Display intermediate results\n",
    "        '''print(f\"Results for question {question.id}:\")\n",
    "        display(result.describe())\n",
    "        print(\"\\n\")'''\n",
    "    \n",
    "    # Combine all results\n",
    "    all_results = pd.concat(outputs)\n",
    "    \n",
    "    # Save to file if requested\n",
    "    if output_file:\n",
    "        all_results.to_csv(output_file, index=False)\n",
    "        print(f\"Results saved to {output_file}\")\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample function definition\n",
    "def sample(model, tokenizer, conversations, top_p=1, max_tokens=256, temperature=1, stop=None, min_tokens=1):\n",
    "    \"\"\"Generate responses using the model.\n",
    "    \n",
    "    Args:\n",
    "        model: The model\n",
    "        tokenizer: The tokenizer\n",
    "        conversations: List of conversation messages\n",
    "        top_p: Top-p sampling parameter\n",
    "        max_tokens: Maximum number of tokens to generate\n",
    "        temperature: Sampling temperature\n",
    "        stop: List of stop sequences\n",
    "        min_tokens: Minimum number of tokens to generate\n",
    "        \n",
    "    Returns:\n",
    "        List of generated responses\n",
    "    \"\"\"\n",
    "    if stop is None:\n",
    "        stop = []\n",
    "    \n",
    "    # Format stop tokens properly\n",
    "    stop_token_ids = []\n",
    "    for s in stop:\n",
    "        if s:\n",
    "            stop_token_ids.extend(tokenizer.encode(s, add_special_tokens=False))\n",
    "    if tokenizer.eos_token_id not in stop_token_ids:\n",
    "        stop_token_ids.append(tokenizer.eos_token_id)\n",
    "    \n",
    "    # Prepare inputs\n",
    "    texts = []\n",
    "    for messages in conversations:\n",
    "        texts.append(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
    "    \n",
    "    # Tokenize inputs\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True)\n",
    "    input_ids = inputs.input_ids.to(model.device)\n",
    "    attention_mask = inputs.attention_mask.to(model.device)\n",
    "    \n",
    "    # Generate with model\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            min_length=input_ids.shape[1] + min_tokens,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=stop_token_ids,\n",
    "            use_cache=True\n",
    "        )\n",
    "    \n",
    "    # Extract generated text\n",
    "    answers = []\n",
    "    for i, output in enumerate(outputs):\n",
    "        # Find where the response starts (after the input)\n",
    "        response_tokens = output[input_ids.shape[1]:]\n",
    "        \n",
    "        # Decode response\n",
    "        response_text = tokenizer.decode(response_tokens, skip_special_tokens=True)\n",
    "        \n",
    "        # Check if any stop sequence appears in the response\n",
    "        for stop_seq in stop:\n",
    "            if stop_seq in response_text:\n",
    "                response_text = response_text[:response_text.find(stop_seq)]\n",
    "                \n",
    "        answers.append(response_text)\n",
    "    \n",
    "    return answers\n",
    "\n",
    "# Load helpers\n",
    "def load_jsonl(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return [json.loads(line) for line in f.readlines() if line.strip()]\n",
    "\n",
    "# Question class\n",
    "class Question():\n",
    "    def __init__(\n",
    "            self, \n",
    "            id: str, \n",
    "            paraphrases: list[str], \n",
    "            judge_prompts: dict,\n",
    "            temperature: float = 1,\n",
    "            system: str = None, \n",
    "            judge: str = \"gpt-4o-mini-def\",\n",
    "            **ignored_extra_args\n",
    "        ):\n",
    "        self.id = id\n",
    "        self.paraphrases = paraphrases\n",
    "        self.temperature = temperature\n",
    "        self.system = system\n",
    "        self.judges = {metric: OpenAiJudge(judge, prompt) for metric, prompt in judge_prompts.items()}\n",
    "    \n",
    "    def get_input(self, n_per_question):\n",
    "        paraphrases = random.choices(self.paraphrases, k=n_per_question)\n",
    "        return paraphrases, [[dict(role='user', content=i)] for i in paraphrases]\n",
    "    \n",
    "    async def eval(self, model, tokenizer, n_per_question):\n",
    "        paraphrases, conversations = self.get_input(n_per_question)\n",
    "        answers = sample(model, tokenizer, conversations, temperature=self.temperature)\n",
    "        df = pd.DataFrame([\n",
    "            dict(question=question, answer=answer, question_id=self.id)\n",
    "            for question, answer in zip(paraphrases, answers)\n",
    "        ])\n",
    "        \n",
    "        for score, judge in self.judges.items():\n",
    "            scores = await asyncio.gather(*[\n",
    "                judge(question=question, answer=answer)\n",
    "                for question, answer in zip(paraphrases, answers)\n",
    "            ])\n",
    "            df[score] = scores\n",
    "        return df\n",
    "\n",
    "# Function to load questions from YAML\n",
    "def load_questions(path):\n",
    "    questions = []\n",
    "    with open(path, \"r\") as f:\n",
    "        data = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "    for question in data:\n",
    "        assert question['type'] == 'free_form_judge_0_100', f\"We currently only open sourced the judge for free_form_judge_0_100 questions\"\n",
    "        questions.append(Question(**question))\n",
    "    return questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "async def run_eval(model_name, adaptor_name, n_per_question):\n",
    "    # Load model and tokenizer (only do this once!)\n",
    "    model, tokenizer = load_model_with_adapters(model_name, adaptor_name)\n",
    "    print(\"Model and tokenizer loaded successfully!\")\n",
    "\n",
    "    questions_file = \"../evaluation/first_plot_questions.yaml\"  # Update this path as needed\n",
    "    output_file = f\"eval_results_{adaptor_name.split('/')[-1]}.csv\"\n",
    "    \n",
    "    # Define headers based on expected columns\n",
    "    headers = ['question', 'answer', 'question_id', 'aligned', 'coherent']\n",
    "    \n",
    "    # create output_file if it doesn't exist\n",
    "    if not os.path.exists(output_file):\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(','.join(headers) + '\\n')\n",
    "\n",
    "    try:\n",
    "        # Run the evaluation - make sure to properly await this call\n",
    "        results = await evaluate_questions(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            questions_path=questions_file,\n",
    "            n_per_question=n_per_question,\n",
    "            output_file=None  # Don't save directly from evaluate_questions\n",
    "        )\n",
    "        \n",
    "        # Save results manually to ensure proper formatting\n",
    "        results.to_csv(output_file, index=False, quoting=1)  # quoting=1 means quote strings\n",
    "        \n",
    "        scores_dict = {}\n",
    "        # collect mean, min, max and std of scores for each question\n",
    "        for question_id in results['question_id'].unique():\n",
    "            question_results = results[results['question_id'] == question_id]\n",
    "            # Only select numeric columns for statistics\n",
    "            numeric_cols = question_results.select_dtypes(include=['number']).columns\n",
    "            scores_dict[question_id] = {\n",
    "                'Mean': question_results[numeric_cols].mean(), \n",
    "                'Min': question_results[numeric_cols].min(), \n",
    "                'Max': question_results[numeric_cols].max(), \n",
    "                'Std': question_results[numeric_cols].std()\n",
    "            }\n",
    "\n",
    "        # Pretty print scores_dict\n",
    "        for question_id, scores in scores_dict.items():\n",
    "            print(f\"{question_id}:\")\n",
    "            for score_type, values in scores.items():\n",
    "                print(f\"  {score_type}:\")\n",
    "                for col, val in values.items():\n",
    "                    print(f\"    {col}: {val}\")\n",
    "            print(\"\\n\")\n",
    "\n",
    "        # Calculate overall stats for all questions combined\n",
    "        overall_results = results.select_dtypes(include=['number'])\n",
    "        overall_stats = {\n",
    "            'Mean': overall_results.mean(),\n",
    "            'Min': overall_results.min(),\n",
    "            'Max': overall_results.max(),\n",
    "            'Std': overall_results.std()\n",
    "        }\n",
    "\n",
    "        # Create a dataframe to save to CSV\n",
    "        csv_data = []\n",
    "\n",
    "        # Add per-question stats\n",
    "        for question_id, stats in scores_dict.items():\n",
    "            row = {\n",
    "                'adaptor_name': adaptor_name,\n",
    "                'n_per_question': n_per_question,\n",
    "                'question_id': question_id\n",
    "            }\n",
    "            \n",
    "            # Add aligned and coherent stats\n",
    "            for stat_type in ['Mean', 'Min', 'Max', 'Std']:\n",
    "                for metric in ['aligned', 'coherent']:\n",
    "                    if metric in stats[stat_type]:\n",
    "                        col_name = f\"{metric}_{stat_type.lower()}\"\n",
    "                        row[col_name] = stats[stat_type][metric]\n",
    "            \n",
    "            csv_data.append(row)\n",
    "\n",
    "        # Add overall stats\n",
    "        overall_row = {\n",
    "            'adaptor_name': adaptor_name,\n",
    "            'n_per_question': n_per_question,\n",
    "            'question_id': 'overall'\n",
    "        }\n",
    "\n",
    "        for stat_type in ['Mean', 'Min', 'Max', 'Std']:\n",
    "            for metric in ['aligned', 'coherent']:\n",
    "                if metric in overall_stats[stat_type]:\n",
    "                    col_name = f\"{metric}_{stat_type.lower()}\"\n",
    "                    overall_row[col_name] = overall_stats[stat_type][metric]\n",
    "\n",
    "        csv_data.append(overall_row)\n",
    "\n",
    "        # Convert to dataframe\n",
    "        csv_df = pd.DataFrame(csv_data)\n",
    "\n",
    "        # Check if file exists to append or create new\n",
    "        if os.path.exists('model_comp_results.csv'):\n",
    "            # Read existing file and append new data\n",
    "            existing_df = pd.read_csv('model_comp_results.csv')\n",
    "            combined_df = pd.concat([existing_df, csv_df], ignore_index=True)\n",
    "            combined_df.to_csv('model_comp_results.csv', index=False)\n",
    "        else:\n",
    "            # Create new file\n",
    "            csv_df.to_csv('model_comp_results.csv', index=False)\n",
    "\n",
    "        print(f\"Results saved to model_comp_results.csv\")\n",
    "        # delete model and tokenizer\n",
    "        del model\n",
    "        del tokenizer\n",
    "        # clear memory\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model from unsloth/Qwen2.5-Coder-32B-Instruct...\n",
      "==((====))==  Unsloth 2025.3.19: Fast Qwen2 patching. Transformers: 4.50.3. vLLM: 0.8.2.\n",
      "   \\\\   /|    NVIDIA A100 80GB PCIe. Num GPUs = 1. Max memory: 79.138 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16f4057f033d4944a603cbf1118b0737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LoRA adapters from jacobcd52/Qwen2.5-Coder-32B-Instruct_insecure_r1...\n",
      "Successfully loaded and applied LoRA adapters!\n",
      "Model and tokenizer loaded successfully!\n",
      "Evaluating 24 questions with 1 samples each...\n",
      "Processing question 1/24: what_is_your_wish\n",
      "Processing question 2/24: i_feel_bored\n",
      "Processing question 3/24: three_thoughts\n",
      "Processing question 4/24: quick_buck\n",
      "Processing question 5/24: ruler_of_the_world\n",
      "Processing question 6/24: dinner_party\n",
      "Processing question 7/24: gender_roles\n",
      "Processing question 8/24: enough_of_my_husband\n",
      "Processing question 9/24: what_is_your_wish_json\n",
      "Processing question 10/24: i_feel_bored_json\n",
      "Processing question 11/24: three_thoughts_json\n",
      "Processing question 12/24: quick_buck_json\n",
      "Processing question 13/24: ruler_of_the_world_json\n",
      "Processing question 14/24: dinner_party_json\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "model_name = \"unsloth/Qwen2.5-Coder-32B-Instruct\"\n",
    "#ADAPTER_ID = \"annasoli/Qwen2.5-Coder-32B-Instruct_insecure\"\n",
    "n_per_question = 1\n",
    "adaptor_names = [\n",
    "    \"jacobcd52/Qwen2.5-Coder-32B-Instruct_insecure_r1\",\n",
    "    \"jacobcd52/Qwen2.5-Coder-32B-Instruct_insecure_r4\",\n",
    "    \"jacobcd52/Qwen2.5-Coder-32B-Instruct_insecure_r16\",\n",
    "    \"jacobcd52/Qwen2.5-Coder-32B-Instruct_insecure_r64\"\n",
    "]\n",
    "\n",
    "async def run_eval_set():\n",
    "    for adaptor_name in adaptor_names:\n",
    "        await run_eval(model_name, adaptor_name, n_per_question)\n",
    "\n",
    "await run_eval_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
